import asyncio
import time
import json
import requests
from typing import Dict, List, Any
from datetime import datetime
import pandas as pd
import logging


class EnhancedPoCEvaluator:
    def __init__(self, backend_url: str = "http://localhost:8100"):
        self.backend_url = backend_url
        self.test_results = {}
        self.performance_data = []

        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def call_api(self, endpoint: str, data: Dict = None) -> Dict:
        """API í˜¸ì¶œ with timeout and retry"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if data:
                    response = requests.post(
                        f"{self.backend_url}{endpoint}", json=data, timeout=30
                    )
                else:
                    response = requests.get(f"{self.backend_url}{endpoint}", timeout=30)

                if response.status_code == 200:
                    return response.json()
                else:
                    self.logger.error(f"HTTP {response.status_code}: {response.text}")
                    return {"error": f"HTTP {response.status_code}"}

            except Exception as e:
                if attempt == max_retries - 1:
                    return {"error": str(e)}
                time.sleep(2**attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

        return {"error": "Max retries exceeded"}

    def test_procedural_memory_flow_mode(self):
        """ì ˆì°¨ì  ë©”ëª¨ë¦¬ - í”Œë¡œìš° ëª¨ë“œ ê²€ì¦"""
        self.logger.info("ğŸ§  ì ˆì°¨ì  ë©”ëª¨ë¦¬ (í”Œë¡œìš° ëª¨ë“œ) í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        test_case = {
            "message": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ SHE ë¹„ìƒ ì •ë³´ ì¡°íšŒí•˜ê³  Slackìœ¼ë¡œ ì•Œë¦¼í•´ì£¼ì„¸ìš”",
            "user_id": "test_procedural_user",
            "mode": "flow",
            "expected_tools": ["SEARCH_DB", "GENERATE_MSG", "SEND_SLACK"],
        }

        results = []

        # 3ë²ˆ ë°˜ë³µ ì‹¤í–‰ìœ¼ë¡œ íŒ¨í„´ í•™ìŠµ í™•ì¸
        for iteration in range(3):
            session_id = f"procedural_flow_test_{iteration}"

            start_time = time.time()
            response = self.call_api(
                "/chat",
                {
                    "message": test_case["message"],
                    "user_id": test_case["user_id"],
                    "session_id": session_id,
                    "mode": test_case["mode"],
                },
            )
            end_time = time.time()

            if "error" not in response:
                used_tools = [
                    tool["tool_type"] for tool in response.get("tools_used", [])
                ]
                workflow_executed = response.get("workflow_executed")

                result = {
                    "iteration": iteration + 1,
                    "processing_time": end_time - start_time,
                    "tools_used": used_tools,
                    "workflow_pattern": workflow_executed is not None,
                    "success": len(used_tools) >= 2,  # ìµœì†Œ 2ê°œ ë„êµ¬ ì‚¬ìš©
                    "pattern_reuse": iteration > 0 and workflow_executed is not None,
                }

                results.append(result)
                self.logger.info(
                    f"  ë°˜ë³µ {iteration + 1}: {result['processing_time']:.2f}ì´ˆ - {'âœ…' if result['success'] else 'âŒ'}"
                )
            else:
                results.append(
                    {
                        "iteration": iteration + 1,
                        "error": response["error"],
                        "success": False,
                    }
                )

        # ì„±ëŠ¥ ê°œì„  í™•ì¸ (ì‹œê°„ ë‹¨ì¶•)
        if len(results) >= 3:
            time_improvement = (
                results[0]["processing_time"] - results[-1]["processing_time"]
            )
            pattern_learning = sum(
                1 for r in results[1:] if r.get("pattern_reuse", False)
            )

            success_metrics = {
                "total_tests": len(results),
                "successful_tests": sum(1 for r in results if r.get("success", False)),
                "success_rate": sum(1 for r in results if r.get("success", False))
                / len(results),
                "time_improvement": time_improvement,
                "pattern_learning_rate": (
                    pattern_learning / (len(results) - 1) if len(results) > 1 else 0
                ),
                "avg_processing_time": sum(r.get("processing_time", 0) for r in results)
                / len(results),
            }

            return {
                "test_name": "ì ˆì°¨ì  ë©”ëª¨ë¦¬ - í”Œë¡œìš° ëª¨ë“œ",
                "results": results,
                "metrics": success_metrics,
                "passed": success_metrics["success_rate"] >= 0.8
                and success_metrics["pattern_learning_rate"] >= 0.5,
            }

        return {
            "test_name": "ì ˆì°¨ì  ë©”ëª¨ë¦¬ - í”Œë¡œìš° ëª¨ë“œ",
            "results": results,
            "passed": False,
        }

    def test_episodic_memory_personalization(self):
        """ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ - ê°œì¸í™” í•™ìŠµ ê²€ì¦"""
        self.logger.info("ğŸ“š ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ ê°œì¸í™” í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        user_id = "test_episodic_user"

        # 1ë‹¨ê³„: ì„ í˜¸ë„ í•™ìŠµ
        preference_session = f"episodic_pref_{int(time.time())}"
        preference_response = self.call_api(
            "/chat",
            {
                "message": "ì €ëŠ” ê¸°ìˆ ì ì¸ ì„¤ëª…ì„ ì„ í˜¸í•˜ê³ , ê°„ê²°í•œ ë©”ì‹œì§€ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤",
                "user_id": user_id,
                "session_id": preference_session,
                "mode": "basic",
            },
        )

        # í”¼ë“œë°±ìœ¼ë¡œ ì„ í˜¸ë„ ê°•í™”
        feedback_response = self.call_api(
            "/feedback",
            {
                "session_id": preference_session,
                "user_id": user_id,
                "feedback_type": "style_preference",
                "content": "ê¸°ìˆ ì ì´ê³  ê°„ê²°í•œ ìŠ¤íƒ€ì¼ì„ ì„ í˜¸í•©ë‹ˆë‹¤",
                "rating": 5.0,
            },
        )

        time.sleep(2)  # ë©”ëª¨ë¦¬ ì²˜ë¦¬ ì‹œê°„ ëŒ€ê¸°

        # 2ë‹¨ê³„: ìƒˆ ì„¸ì…˜ì—ì„œ ê°œì¸í™” ì ìš© í™•ì¸
        test_session = f"episodic_test_{int(time.time())}"
        test_response = self.call_api(
            "/chat",
            {
                "message": "ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "user_id": user_id,
                "session_id": test_session,
                "mode": "basic",
            },
        )

        # 3ë‹¨ê³„: ì„ í˜¸ë„ ì¡°íšŒë¡œ í•™ìŠµ í™•ì¸
        preferences_response = self.call_api(f"/user/{user_id}/preferences")

        results = {
            "preference_learning": "error" not in preference_response,
            "feedback_processing": "error" not in feedback_response
            and feedback_response.get("applied", False),
            "personalization_applied": "error" not in test_response
            and len(test_response.get("memory_used", {}).get("EPISODIC", [])) > 0,
            "preferences_stored": "error" not in preferences_response
            and len(preferences_response) > 0,
            "processing_times": {
                "preference": preference_response.get("processing_time", 0),
                "feedback": feedback_response.get("processing_time", 0),
                "personalized": test_response.get("processing_time", 0),
            },
        }

        success_rate = (
            sum(1 for k, v in results.items() if k != "processing_times" and v) / 4
        )

        return {
            "test_name": "ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ - ê°œì¸í™” í•™ìŠµ",
            "results": results,
            "metrics": {
                "success_rate": success_rate,
                "preference_recall": results["personalization_applied"],
                "feedback_responsiveness": results["feedback_processing"],
            },
            "passed": success_rate >= 0.75,
        }

    def test_5_second_feedback_target(self):
        """5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬ ëª©í‘œ ê²€ì¦"""
        self.logger.info("âš¡ 5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        feedback_tests = [
            {"type": "style_preference", "content": "ë” ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”"},
            {
                "type": "response_quality",
                "content": "ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ìš”",
                "rating": 2.0,
            },
            {"type": "tool_performance", "content": "ë°ì´í„° ì¡°íšŒê°€ ëŠë ¤ìš”"},
            {"type": "workflow_efficiency", "content": "ë‹¨ê³„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”"},
            {"type": "user_experience", "content": "ë” ì§ê´€ì ì´ì—ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”"},
        ]

        results = []

        for i, test in enumerate(feedback_tests):
            session_id = f"feedback_speed_test_{i}"

            start_time = time.time()
            response = self.call_api(
                "/feedback",
                {
                    "session_id": session_id,
                    "user_id": "feedback_speed_tester",
                    "feedback_type": test["type"],
                    "content": test["content"],
                    "rating": test.get("rating"),
                },
            )
            end_time = time.time()

            processing_time = end_time - start_time
            meets_target = processing_time < 5.0

            result = {
                "test_case": i + 1,
                "feedback_type": test["type"],
                "processing_time": processing_time,
                "meets_5s_target": meets_target,
                "applied": response.get("applied", False),
                "optimizations_count": len(response.get("optimizations", [])),
            }

            results.append(result)

            status = "âœ…" if meets_target else "âŒ"
            self.logger.info(f"  í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.3f}ì´ˆ - {status}")

        # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        target_achievement_rate = sum(1 for r in results if r["meets_5s_target"]) / len(
            results
        )
        avg_processing_time = sum(r["processing_time"] for r in results) / len(results)
        application_rate = sum(1 for r in results if r["applied"]) / len(results)

        return {
            "test_name": "5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬",
            "results": results,
            "metrics": {
                "target_achievement_rate": target_achievement_rate,
                "avg_processing_time": avg_processing_time,
                "application_rate": application_rate,
                "total_tests": len(results),
            },
            "passed": target_achievement_rate >= 0.95,  # 95% ëª©í‘œ
        }

    def test_cross_agent_learning(self):
        """í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ê²€ì¦"""
        self.logger.info("ğŸ¤ í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        user_id = "cross_agent_test_user"

        # Agent 1ì—ì„œ í•™ìŠµ
        agent1_session = f"cross_agent_1_{int(time.time())}"

        # ì„ í˜¸ë„ ì„¤ì •
        learning_response = self.call_api(
            "/chat",
            {
                "message": "ì €ëŠ” í•­ìƒ Slack ì•Œë¦¼ì„ ì„ í˜¸í•˜ê³ , ìƒì„¸í•œ ë¡œê·¸ ì •ë³´ë¥¼ ì›í•©ë‹ˆë‹¤",
                "user_id": user_id,
                "session_id": agent1_session,
                "mode": "basic",
            },
        )

        # í”¼ë“œë°±ìœ¼ë¡œ í•™ìŠµ ê°•í™”
        feedback_response = self.call_api(
            "/feedback",
            {
                "session_id": agent1_session,
                "user_id": user_id,
                "feedback_type": "style_preference",
                "content": "ìƒì„¸í•œ ê¸°ìˆ  ì •ë³´ë¥¼ í¬í•¨í•´ì„œ Slackìœ¼ë¡œ ì•Œë¦¼í•´ì£¼ì„¸ìš”",
                "rating": 5.0,
            },
        )

        time.sleep(3)  # í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ì „íŒŒ ì‹œê°„

        # Agent 2ì—ì„œ í•™ìŠµ ë‚´ìš© í™œìš© í™•ì¸
        agent2_session = f"cross_agent_2_{int(time.time())}"

        application_response = self.call_api(
            "/chat",
            {
                "message": "ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                "user_id": user_id,  # ë™ì¼í•œ ì‚¬ìš©ì
                "session_id": agent2_session,  # ë‹¤ë¥¸ ì„¸ì…˜ (ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
                "mode": "basic",
            },
        )

        # ì„ í˜¸ë„ ë°ì´í„° í™•ì¸
        preferences_response = self.call_api(f"/user/{user_id}/preferences")

        results = {
            "initial_learning": "error" not in learning_response,
            "feedback_applied": feedback_response.get("applied", False),
            "cross_agent_memory_used": len(
                application_response.get("memory_used", {}).get("EPISODIC", [])
            )
            > 0,
            "preferences_shared": len(preferences_response) > 0,
            "processing_times": {
                "learning": learning_response.get("processing_time", 0),
                "feedback": feedback_response.get("processing_time", 0),
                "application": application_response.get("processing_time", 0),
            },
            "tools_optimization": any(
                "slack" in tool.get("tool_type", "").lower()
                for tool in application_response.get("tools_used", [])
            ),
        }

        # ì„±ê³µë¥  ê³„ì‚°
        success_indicators = [
            "initial_learning",
            "feedback_applied",
            "cross_agent_memory_used",
            "preferences_shared",
        ]
        success_rate = sum(
            1 for indicator in success_indicators if results[indicator]
        ) / len(success_indicators)

        return {
            "test_name": "í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ",
            "results": results,
            "metrics": {
                "success_rate": success_rate,
                "learning_transfer_success": results["cross_agent_memory_used"],
                "preference_sharing_success": results["preferences_shared"],
            },
            "passed": success_rate >= 0.8,  # 80% ëª©í‘œ
        }

    def test_mcp_tool_performance_optimization(self):
        """MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™” ê²€ì¦"""
        self.logger.info("ğŸ”§ MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘...")

        # ì´ˆê¸° ì„±ëŠ¥ ì¸¡ì •
        initial_stats = self.call_api("/mcp/tools/performance")

        # ì—¬ëŸ¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        test_workflows = [
            {
                "message": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ Slackìœ¼ë¡œ ì „ì†¡í•´ì£¼ì„¸ìš”",
                "expected_tools": ["SEARCH_DB", "SEND_SLACK"],
            },
            {
                "message": "ë¹„ìƒ ìƒí™© ì•Œë¦¼ì„ ìƒì„±í•˜ê³  ì´ë©”ì¼ê³¼ Slackìœ¼ë¡œ ë™ì‹œ ë°œì†¡í•´ì£¼ì„¸ìš”",
                "expected_tools": ["EMERGENCY_MAIL", "SEND_SLACK"],
            },
            {
                "message": "ì‹œìŠ¤í…œ ìƒíƒœ ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì„œ ê´€ë¦¬ìì—ê²Œ ì „ë‹¬í•´ì£¼ì„¸ìš”",
                "expected_tools": ["GENERATE_MSG", "SEND_SLACK"],
            },
        ]

        performance_results = []

        for i, workflow in enumerate(test_workflows):
            session_id = f"mcp_perf_test_{i}"

            start_time = time.time()
            response = self.call_api(
                "/chat",
                {
                    "message": workflow["message"],
                    "user_id": "mcp_performance_tester",
                    "session_id": session_id,
                    "mode": "flow",
                },
            )
            end_time = time.time()

            if "error" not in response:
                tools_used = response.get("tools_used", [])
                successful_tools = [t for t in tools_used if t.get("success")]

                result = {
                    "workflow": i + 1,
                    "total_time": end_time - start_time,
                    "tools_used": len(tools_used),
                    "successful_tools": len(successful_tools),
                    "success_rate": (
                        len(successful_tools) / len(tools_used) if tools_used else 0
                    ),
                    "avg_tool_time": (
                        sum(t.get("execution_time", 0) for t in tools_used)
                        / len(tools_used)
                        if tools_used
                        else 0
                    ),
                }

                performance_results.append(result)
                self.logger.info(
                    f"  ì›Œí¬í”Œë¡œìš° {i+1}: {result['total_time']:.2f}ì´ˆ, ì„±ê³µë¥  {result['success_rate']:.1%}"
                )

        # ìµœì¢… ì„±ëŠ¥ ì¸¡ì •
        final_stats = self.call_api("/mcp/tools/performance")

        # ì„±ëŠ¥ ê°œì„  í™•ì¸
        improvements = {}
        if initial_stats and final_stats:
            for tool_name in initial_stats.keys():
                if tool_name in final_stats:
                    initial_time = initial_stats[tool_name].get(
                        "recent_avg_time",
                        initial_stats[tool_name].get("avg_response_time", 0),
                    )
                    final_time = final_stats[tool_name].get(
                        "recent_avg_time",
                        final_stats[tool_name].get("avg_response_time", 0),
                    )

                    if initial_time > 0:
                        improvements[tool_name] = {
                            "time_improvement": initial_time - final_time,
                            "improvement_rate": (
                                (initial_time - final_time) / initial_time
                                if initial_time > 0
                                else 0
                            ),
                        }

        metrics = {
            "total_workflows": len(performance_results),
            "avg_success_rate": (
                sum(r["success_rate"] for r in performance_results)
                / len(performance_results)
                if performance_results
                else 0
            ),
            "avg_processing_time": (
                sum(r["total_time"] for r in performance_results)
                / len(performance_results)
                if performance_results
                else 0
            ),
            "performance_improvements": improvements,
        }

        return {
            "test_name": "MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™”",
            "results": performance_results,
            "metrics": metrics,
            "passed": metrics["avg_success_rate"] >= 0.85
            and metrics["avg_processing_time"] <= 5.0,
        }

    def run_comprehensive_evaluation(self):
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        self.logger.info("ğŸš€ Enhanced Agentic AI PoC ì¢…í•© í‰ê°€ ì‹œì‘...")
        self.logger.info("=" * 60)

        evaluation_results @ app.get("/feedback/optimization-history")


async def optimization_history_endpoint():
    """ìµœì í™” ì´ë ¥ ì¡°íšŒ"""
    try:
        history = await feedback_service.get_optimization_history()
        return history
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìµœì í™” ì´ë ¥ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")


@app.get("/user/{user_id}/preferences")
async def user_preferences_endpoint(user_id: str):
    """ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ"""
    try:
        preferences = await feedback_service.get_user_preferences(user_id)
        return preferences
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„ í˜¸ë„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "services": {
            "memory": memory_service is not None,
            "mcp": mcp_service is not None,
            "feedback": feedback_service is not None,
            "agent": agent_service is not None,
        },
        "version": "2.0.0",
    }


@app.get("/metrics")
async def metrics():
    """Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
