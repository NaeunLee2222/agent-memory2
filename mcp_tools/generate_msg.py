import asyncio
import time
import random
from typing import Dict, Any
from openai import OpenAI
from ..models.schemas import MCPToolResult, MCPToolType
from ..utils.config import config


class GenerateMessageTool:
    def __init__(self):
        self.tool_type = MCPToolType.GENERATE_MSG
        self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.performance_stats = {
            "avg_response_time": 2.1,
            "success_rate": 0.94,
            "template_cache": {},
        }

    async def execute(
        self, parameters: Dict[str, Any], context: Dict[str, Any] = None
    ) -> MCPToolResult:
        """ë©”ì‹œì§€ ìƒì„± ë„êµ¬ ì‹¤í–‰"""
        start_time = time.time()

        message_type = parameters.get("type", "general")
        content = parameters.get("content", "")
        style = parameters.get(
            "style", "professional"
        )  # professional, casual, technical
        length = parameters.get("length", "medium")  # short, medium, long

        try:
            # í…œí”Œë¦¿ ìºì‹± ìµœì í™” ì‹œë®¬ë ˆì´ì…˜
            cache_key = f"{message_type}_{style}_{length}"
            if cache_key in self.performance_stats["template_cache"]:
                execution_time = 1.1 + random.uniform(0, 0.2)  # ìºì‹œ ì ìš©ìœ¼ë¡œ ë¹ ë¦„
            else:
                execution_time = 2.1 + random.uniform(0, 0.5)  # ìƒˆë¡œ ìƒì„±
                self.performance_stats["template_cache"][cache_key] = True

            await asyncio.sleep(execution_time)

            # ìŠ¤íƒ€ì¼ë³„ ë©”ì‹œì§€ ìƒì„± ë¡œì§
            if style == "professional":
                generated_message = (
                    f"ì•ˆë…•í•˜ì„¸ìš”,\n\n{content}ì— ëŒ€í•´ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.\n\nê°ì‚¬í•©ë‹ˆë‹¤."
                )
            elif style == "casual":
                generated_message = f"ì•ˆë…•! {content} ê´€ë ¨í•´ì„œ ì•Œë ¤ì¤„ê²Œ ğŸ˜Š"
            elif style == "technical":
                generated_message = f"[ì‹œìŠ¤í…œ ì•Œë¦¼] {content}\n\nì„¸ë¶€ ì •ë³´:\n- íƒ€ì„ìŠ¤íƒ¬í”„: {time.strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                generated_message = content

            # ê¸¸ì´ ì¡°ì •
            if length == "short":
                generated_message = (
                    generated_message[:100] + "..."
                    if len(generated_message) > 100
                    else generated_message
                )
            elif length == "long":
                generated_message += (
                    "\n\nì¶”ê°€ì ìœ¼ë¡œ í•„ìš”í•œ ì •ë³´ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”."
                )

            # ì„±ê³µ/ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
            success = random.random() < self.performance_stats["success_rate"]

            if success:
                result_data = {
                    "generated_message": generated_message,
                    "style": style,
                    "length": length,
                    "word_count": len(generated_message.split()),
                    "cached": cache_key in self.performance_stats["template_cache"],
                }

                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=True,
                    result=result_data,
                    execution_time=time.time() - start_time,
                )
            else:
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=False,
                    result=None,
                    execution_time=time.time() - start_time,
                    error="Message generation failed - content policy violation",
                )

        except Exception as e:
            return MCPToolResult(
                tool_type=self.tool_type,
                success=False,
                result=None,
                execution_time=time.time() - start_time,
                error=str(e),
            )

    def get_performance_stats(self) -> Dict[str, Any]:
        return {
            **self.performance_stats,
            "cached_templates": len(self.performance_stats["template_cache"]),
        }
