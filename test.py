evaluation_results = {
            "timestamp": datetime.now().isoformat(),
            "evaluation_version": "2.0",
            "tests": {}
        }
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_methods = [
            self.test_procedural_memory_flow_mode,
            self.test_episodic_memory_personalization,
            self.test_5_second_feedback_target,
            self.test_cross_agent_learning,
            self.test_mcp_tool_performance_optimization
        ]
        
        passed_tests = 0
        total_tests = len(test_methods)
        
        for test_method in test_methods:
            try:
                result = test_method()
                test_name = result["test_name"]
                evaluation_results["tests"][test_name] = result
                
                if result.get("passed", False):
                    passed_tests += 1
                    self.logger.info(f"âœ… {test_name}: í†µê³¼")
                else:
                    self.logger.info(f"âŒ {test_name}: ì‹¤íŒ¨")
                    
            except Exception as e:
                self.logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                evaluation_results["tests"][f"error_{test_method.__name__}"] = {
                    "error": str(e),
                    "passed": False
                }
        
        # ì¢…í•© í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        evaluation_results["overall_metrics"] = self._calculate_overall_metrics(evaluation_results)
        
        # ê²°ê³¼ ì €ì¥
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"enhanced_poc_evaluation_{timestamp_str}.json"
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False, default=str)
        
        # í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_evaluation_report(evaluation_results, passed_tests, total_tests)
        
        return evaluation_results
    
    def _calculate_overall_metrics(self, results: Dict) -> Dict:
        """ì¢…í•© ì„±ê³¼ ì§€í‘œ ê³„ì‚°"""
        metrics = {}
        
        # ì ˆì°¨ì  ë©”ëª¨ë¦¬ ì„±ëŠ¥
        procedural_test = results["tests"].get("ì ˆì°¨ì  ë©”ëª¨ë¦¬ - í”Œë¡œìš° ëª¨ë“œ", {})
        if procedural_test:
            proc_metrics = procedural_test.get("metrics", {})
            metrics["procedural_memory_success_rate"] = proc_metrics.get("success_rate", 0)
            metrics["workflow_learning_rate"] = proc_metrics.get("pattern_learning_rate", 0)
        
        # í”¼ë“œë°± ë£¨í”„ ì„±ëŠ¥
        feedback_test = results["tests"].get("5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬", {})
        if feedback_test:
            fb_metrics = feedback_test.get("metrics", {})
            metrics["feedback_target_achievement"] = fb_metrics.get("target_achievement_rate", 0)
            metrics["avg_feedback_time"] = fb_metrics.get("avg_processing_time", 0)
        
        # ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ ì„±ëŠ¥
        episodic_test = results["tests"].get("ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ - ê°œì¸í™” í•™ìŠµ", {})
        if episodic_test:
            ep_metrics = episodic_test.get("metrics", {})
            metrics["personalization_success_rate"] = ep_metrics.get("success_rate", 0)
        
        # í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ì„±ëŠ¥
        cross_test = results["tests"].get("í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ", {})
        if cross_test:
            cross_metrics = cross_test.get("metrics", {})
            metrics["cross_agent_success_rate"] = cross_metrics.get("success_rate", 0)
        
        # MCP ë„êµ¬ ì„±ëŠ¥
        mcp_test = results["tests"].get("MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™”", {})
        if mcp_test:
            mcp_metrics = mcp_test.get("metrics", {})
            metrics["mcp_tool_success_rate"] = mcp_metrics.get("avg_success_rate", 0)
            metrics["avg_mcp_processing_time"] = mcp_metrics.get("avg_processing_time", 0)
        
        # ì „ì²´ ì„±ê³µë¥ 
        passed_tests = sum(1 for test in results["tests"].values() if test.get("passed", False))
        total_tests = len(results["tests"])
        metrics["overall_success_rate"] = passed_tests / total_tests if total_tests > 0 else 0
        
        return metrics
    
    def _generate_evaluation_report(self, results: Dict, passed_tests: int, total_tests: int):
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸ“‹ Enhanced Agentic AI PoC í‰ê°€ ë¦¬í¬íŠ¸")
        self.logger.info("=" * 60)
        
        overall_metrics = results.get("overall_metrics", {})
        
        # í•µì‹¬ ì„±ê³¼ ì§€í‘œ
        self.logger.info("\nğŸ¯ í•µì‹¬ ì„±ê³¼ ì§€í‘œ:")
        self.logger.info("-" * 30)
        
        # 1. ì ˆì°¨ì  ë©”ëª¨ë¦¬ (ì›Œí¬í”Œë¡œìš° í•™ìŠµ)
        proc_success = overall_metrics.get("procedural_memory_success_rate", 0)
        workflow_learning = overall_metrics.get("workflow_learning_rate", 0)
        self.logger.info(f"ì›Œí¬í”Œë¡œìš° í•™ìŠµ ì„±ê³µë¥ : {proc_success:.1%} (ëª©í‘œ: >80%)")
        self.logger.info(f"íŒ¨í„´ í•™ìŠµë¥ : {workflow_learning:.1%} (ëª©í‘œ: >50%)")
        
        # 2. 5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬
        feedback_achievement = overall_metrics.get("feedback_target_achievement", 0)
        avg_feedback_time = overall_metrics.get("avg_feedback_time", 0)
        self.logger.info(f"5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬ìœ¨: {feedback_achievement:.1%} (ëª©í‘œ: >95%)")
        self.logger.info(f"í‰ê·  í”¼ë“œë°± ì²˜ë¦¬ ì‹œê°„: {avg_feedback_time:.3f}ì´ˆ (ëª©í‘œ: <5ì´ˆ)")
        
        # 3. ê°œì¸í™” í•™ìŠµ
        personalization = overall_metrics.get("personalization_success_rate", 0)
        self.logger.info(f"ê°œì¸í™” í•™ìŠµ ì„±ê³µë¥ : {personalization:.1%} (ëª©í‘œ: >75%)")
        
        # 4. í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ
        cross_agent = overall_metrics.get("cross_agent_success_rate", 0)
        self.logger.info(f"í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ì„±ê³µë¥ : {cross_agent:.1%} (ëª©í‘œ: >80%)")
        
        # 5. MCP ë„êµ¬ ì„±ëŠ¥
        mcp_success = overall_metrics.get("mcp_tool_success_rate", 0)
        mcp_time = overall_metrics.get("avg_mcp_processing_time", 0)
        self.logger.info(f"MCP ë„êµ¬ ì„±ê³µë¥ : {mcp_success:.1%} (ëª©í‘œ: >85%)")
        self.logger.info(f"í‰ê·  MCP ì²˜ë¦¬ ì‹œê°„: {mcp_time:.2f}ì´ˆ (ëª©í‘œ: <5ì´ˆ)")
        
        # ì „ì²´ ì„±ê³µë¥ 
        overall_success = overall_metrics.get("overall_success_rate", 0)
        self.logger.info(f"\nğŸ† ì „ì²´ ì„±ê³µë¥ : {overall_success:.1%} ({passed_tests}/{total_tests} í…ŒìŠ¤íŠ¸ í†µê³¼)")
        
        # PoC ì„±ê³µ ì—¬ë¶€ íŒì •
        self.logger.info("\nğŸ‰ PoC ì„±ê³µ ê¸°ì¤€ í‰ê°€:")
        self.logger.info("-" * 30)
        
        success_criteria = [
            ("ì›Œí¬í”Œë¡œìš° í•™ìŠµ", proc_success >= 0.8, proc_success),
            ("5ì´ˆ ì´ë‚´ í”¼ë“œë°±", feedback_achievement >= 0.95, feedback_achievement),
            ("ê°œì¸í™” í•™ìŠµ", personalization >= 0.75, personalization),
            ("í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ", cross_agent >= 0.8, cross_agent),
            ("MCP ë„êµ¬ ì„±ëŠ¥", mcp_success >= 0.85, mcp_success)
        ]
        
        passed_criteria = 0
        for criteria_name, passed, value in success_criteria:
            status = "âœ… í†µê³¼" if passed else "âŒ ë¯¸ë‹¬ì„±"
            self.logger.info(f"{criteria_name}: {status} ({value:.1%})")
            if passed:
                passed_criteria += 1
        
        final_success_rate = passed_criteria / len(success_criteria)
        
        self.logger.info(f"\nğŸ“Š ê¸°ì¤€ ë‹¬ì„±ë¥ : {final_success_rate:.1%} ({passed_criteria}/{len(success_criteria)})")
        
        # ìµœì¢… íŒì •
        if final_success_rate >= 0.8:
            self.logger.info("ğŸ‰ PoC ì„±ê³µ! - ìš´ì˜ í™˜ê²½ ì ìš© ê°€ëŠ¥")
        elif final_success_rate >= 0.6:
            self.logger.info("âš ï¸ PoC ë¶€ë¶„ ì„±ê³µ - ì¶”ê°€ ìµœì í™” í•„ìš”")
        else:
            self.logger.info("âŒ PoC ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì¬ì„¤ê³„ ê²€í†  í•„ìš”")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        self.logger.info("\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        self.logger.info("-" * 30)
        
        recommendations = []
        
        if feedback_achievement < 0.95:
            recommendations.append("í”¼ë“œë°± ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìµœì í™” í•„ìš”")
        
        if proc_success < 0.8:
            recommendations.append("ì›Œí¬í”Œë¡œìš° íŒ¨í„´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê°œì„  í•„ìš”")
        
        if cross_agent < 0.8:
            recommendations.append("í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ ê°•í™” í•„ìš”")
        
        if mcp_success < 0.85:
            recommendations.append("MCP ë„êµ¬ ì•ˆì •ì„± ë° ì„±ëŠ¥ ê°œì„  í•„ìš”")
        
        if avg_feedback_time >= 5.0:
            recommendations.append("í”¼ë“œë°± ì²˜ë¦¬ ì†ë„ ìµœì í™” í•„ìš”")
        
        if not recommendations:
            recommendations.append("ëª¨ë“  ëª©í‘œ ë‹¬ì„±! ìš´ì˜ í™˜ê²½ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ")
        
        for i, recommendation in enumerate(recommendations, 1):
            self.logger.info(f"{i}. {recommendation}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced Agentic AI PoC í‰ê°€ ë„êµ¬")
    parser.add_argument("--backend-url", default="http://localhost:8000", 
                       help="ë°±ì—”ë“œ ì„œë²„ URL")
    parser.add_argument("--output-dir", default="./evaluation_results", 
                       help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    evaluator = EnhancedPoCEvaluator(args.backend_url)
    results = evaluator.run_comprehensive_evaluation()

# === ì‹¤í–‰ ê°€ì´ë“œ (README.md) ===
"""
# ğŸ¤– Enhanced Agentic AI PoC - ì‹¤í–‰ ê°€ì´ë“œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
git clone <repository>
cd enhanced-agentic-ai-poc
cp .env.example .env
# .env íŒŒì¼ì— API í‚¤ ì„¤ì •
```

### 2. ì‹œìŠ¤í…œ ì‹œì‘
```bash
docker-compose up --build
```

### 3. ì ‘ì† ì •ë³´
- Frontend: http://localhost:8501
- Backend API: http://localhost:8000
- API ë¬¸ì„œ: http://localhost:8000/docs
- Prometheus: http://localhost:9090

## ğŸ§ª í‰ê°€ ì‹¤í–‰

### ì¢…í•© í‰ê°€
```bash
cd evaluation
pip install requests pandas
python poc_evaluator.py
```

### ê°œë³„ í…ŒìŠ¤íŠ¸
- ì ˆì°¨ì  ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
- ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
- 5ì´ˆ ì´ë‚´ í”¼ë“œë°± í…ŒìŠ¤íŠ¸
- í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ í…ŒìŠ¤íŠ¸
- MCP ë„êµ¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

## ğŸ“Š ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤

### 1. ì ˆì°¨ì  ë©”ëª¨ë¦¬ (í”Œë¡œìš° ëª¨ë“œ)
**ëª©ì **: Step-Action-Tool ì›Œí¬í”Œë¡œìš° íŒ¨í„´ í•™ìŠµ ê²€ì¦
**í…ŒìŠ¤íŠ¸**: ë™ì¼ ì‘ì—… 3íšŒ ë°˜ë³µ â†’ ìë™ íŒ¨í„´ í•™ìŠµ í™•ì¸
**ì„±ê³µ ê¸°ì¤€**: íŒ¨í„´ í•™ìŠµë¥  50% ì´ìƒ, ì²˜ë¦¬ ì‹œê°„ 20% ë‹¨ì¶•

### 2. ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ (ê°œì¸í™”)
**ëª©ì **: ì‚¬ìš©ìë³„ ìƒí˜¸ì‘ìš© í•™ìŠµ ë° ê°œì¸í™” ì„œë¹„ìŠ¤ ê²€ì¦
**í…ŒìŠ¤íŠ¸**: ì„ í˜¸ë„ í•™ìŠµ â†’ ìƒˆ ì„¸ì…˜ì—ì„œ ì ìš© í™•ì¸
**ì„±ê³µ ê¸°ì¤€**: ê°œì¸í™” ì ìš© ì„±ê³µë¥  75% ì´ìƒ

### 3. 5ì´ˆ ì´ë‚´ ì¦‰ì‹œ í”¼ë“œë°±
**ëª©ì **: ì‹¤ì‹œê°„ í”¼ë“œë°± ì²˜ë¦¬ ë° ì‹œìŠ¤í…œ ê°œì„  ê²€ì¦
**í…ŒìŠ¤íŠ¸**: ë‹¤ì–‘í•œ í”¼ë“œë°± ìœ í˜•ë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
**ì„±ê³µ ê¸°ì¤€**: 95% ì´ìƒ 5ì´ˆ ì´ë‚´ ì²˜ë¦¬

### 4. í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ
**ëª©ì **: ì—ì´ì „íŠ¸ ê°„ í•™ìŠµ ê³µìœ  ë° ì§‘ë‹¨ ì§€ëŠ¥ ê²€ì¦
**í…ŒìŠ¤íŠ¸**: A ì—ì´ì „íŠ¸ í•™ìŠµ â†’ B ì—ì´ì „íŠ¸ í™œìš© í™•ì¸
**ì„±ê³µ ê¸°ì¤€**: í•™ìŠµ ì „ì´ ì„±ê³µë¥  80% ì´ìƒ

### 5. MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™”
**ëª©ì **: MCP ê¸°ë°˜ ë„êµ¬ ì²´ì¸ì˜ ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦
**í…ŒìŠ¤íŠ¸**: ë³µí•© ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ â†’ ì„±ëŠ¥ ìµœì í™” í™•ì¸
**ì„±ê³µ ê¸°ì¤€**: ë„êµ¬ ì„±ê³µë¥  85% ì´ìƒ, ì²˜ë¦¬ ì‹œê°„ 5ì´ˆ ì´ë‚´

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### í•„ìˆ˜ ë‹¬ì„± ëª©í‘œ
- âœ… ì›Œí¬í”Œë¡œìš° íŒ¨í„´ í•™ìŠµ: 80% ì´ìƒ
- âœ… 5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬: 95% ì´ìƒ
- âœ… ê°œì¸í™” í•™ìŠµ ì •í™•ë„: 75% ì´ìƒ
- âœ… í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ: 80% ì´ìƒ
- âœ… MCP ë„êµ¬ ì„±ëŠ¥: 85% ì´ìƒ

### ì„±ëŠ¥ ëª©í‘œ
- í‰ê·  ì‘ë‹µ ì‹œê°„: 3ì´ˆ ì´ë‚´
- ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì‹œê°„: 200ms ì´ë‚´
- ì‹œìŠ¤í…œ ê°€ìš©ì„±: 99% ì´ìƒ
- ë™ì‹œ ì‚¬ìš©ì: 100ëª… ì´ìƒ ì§€ì›

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ
1. **ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹¤íŒ¨**: Docker ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ 4GB ì´ìƒ í• ë‹¹
2. **API ì—°ê²° ì˜¤ë¥˜**: ë°±ì—”ë“œ ì™„ì „ ì‹œì‘ ëŒ€ê¸° (30-60ì´ˆ)
3. **ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì˜¤ë¥˜**: Redis/ChromaDB/Neo4j í—¬ìŠ¤ ì²´í¬
4. **ëŠë¦° ì‘ë‹µ**: OpenAI API í‚¤ ë° ìš”ì²­ ì œí•œ í™•ì¸

### ì„±ëŠ¥ ìµœì í™”
- Redis ë©”ëª¨ë¦¬ ì„¤ì • ì¡°ì •
- ChromaDB ì¸ë±ìŠ¤ ìµœì í™”
- Neo4j ê´€ê³„í˜• ì¿¼ë¦¬ ìµœì í™”
- Docker ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì¦ê°€

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### Prometheus ë©”íŠ¸ë¦­
- request_duration_seconds: ìš”ì²­ ì²˜ë¦¬ ì‹œê°„
- memory_operations_total: ë©”ëª¨ë¦¬ ì‘ì—… ìˆ˜
- requests_total: ì´ ìš”ì²­ ìˆ˜

### ì£¼ìš” ë¡œê·¸ ìœ„ì¹˜
- Backend: docker logs enhanced-agentic-ai-poc-backend-1
- Frontend: docker logs enhanced-agentic-ai-poc-frontend-1

## ğŸ“ ì‚¬ìš© ê°€ì´ë“œ

### ê¸°ë³¸ ì‚¬ìš©ë²•
1. ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ì†
2. ì‚¬ìš©ì ID ì„¤ì •
3. ëª¨ë“œ ì„ íƒ (Flow/Basic)
4. ë©”ì‹œì§€ ì…ë ¥ ë° ëŒ€í™”
5. í”¼ë“œë°± ì œê³µìœ¼ë¡œ ì‹œìŠ¤í…œ ê°œì„ 

### ê³ ê¸‰ ê¸°ëŠ¥
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ëª¨ë‹ˆí„°ë§
- ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ìƒíƒœ ë¶„ì„
- í”¼ë“œë°± ì´ë ¥ ë° ìµœì í™” ì¶”ì 
- í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ íš¨ê³¼ í™•ì¸

ì´ Enhanced PoCë¥¼ í†µí•´ ì°¨ì„¸ëŒ€ ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ì‹¤ì¦í•˜ê³  ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""                             'steps': [
                                 {'range': [0, 70], 'color': "lightgray"},
                                 {'range': [70, 90], 'color': "yellow"},
                                 {'range': [90, 100], 'color': "green"}],
                             'threshold': {'line': {'color': "red", 'width': 4},
                                          'thickness': 0.75, 'value': 90}}))
                
                st.plotly_chart(fig_gauge_time, use_container_width=True)
            
            with target_col2:
                # ì‹ ë¢°ë„ ëª©í‘œ (80% ì´ìƒ)
                high_conf = len(df_recent[df_recent["confidence_score"] >= 0.8])
                conf_success_rate = (high_conf / len(df_recent) * 100)
                
                fig_gauge_conf = go.Figure(go.Indicator(
                    mode = "gauge+number+delta",
                    value = conf_success_rate,
                    domain = {'x': [0, 1], 'y': [0, 1]},
                    title = {'text': "ì‹ ë¢°ë„ ëª©í‘œ ë‹¬ì„±ë¥  (%)"},
                    delta = {'reference': 85},
                    gauge = {'axis': {'range': [None, 100]},
                             'bar': {'color': "darkgreen"},
                             'steps': [
                                 {'range': [0, 70], 'color': "lightgray"},
                                 {'range': [70, 85], 'color': "yellow"},
                                 {'range': [85, 100], 'color': "green"}]}))
                
                st.plotly_chart(fig_gauge_conf, use_container_width=True)
            
            with target_col3:
                # ì „ì²´ ì„±ê³µë¥  (ì²˜ë¦¬ ì‹œê°„ + ì‹ ë¢°ë„)
                overall_success = (time_success_rate + conf_success_rate) / 2
                
                fig_gauge_overall = go.Figure(go.Indicator(
                    mode = "gauge+number+delta",
                    value = overall_success,
                    domain = {'x': [0, 1], 'y': [0, 1]},
                    title = {'text': "ì „ì²´ ëª©í‘œ ë‹¬ì„±ë¥  (%)"},
                    delta = {'reference': 80},
                    gauge = {'axis': {'range': [None, 100]},
                             'bar': {'color': "purple"},
                             'steps': [
                                 {'range': [0, 60], 'color': "lightgray"},
                                 {'range': [60, 80], 'color': "yellow"},
                                 {'range': [80, 100], 'color': "green"}]}))
                
                st.plotly_chart(fig_gauge_overall, use_container_width=True)
        
        else:
            st.info("ìµœê·¼ 30ë¶„ê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì±„íŒ…ì„ í†µí•´ ë°ì´í„°ë¥¼ ìƒì„±í•´ë³´ì„¸ìš”.")
    
    else:
        st.info("ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì±„íŒ… ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
    
    # MCP ë„êµ¬ ì„±ëŠ¥ í†µê³„
    st.subheader("ğŸ”§ MCP ë„êµ¬ ì„±ëŠ¥ í†µê³„")
    
    mcp_stats = call_backend("/mcp/tools/performance")
    
    if mcp_stats:
        mcp_data = []
        for tool_name, stats in mcp_stats.items():
            mcp_data.append({
                "ë„êµ¬ëª…": tool_name,
                "ì„±ê³µë¥ ": f"{stats.get('recent_success_rate', stats.get('success_rate', 0)) * 100:.1f}%",
                "í‰ê·  ì‘ë‹µì‹œê°„": f"{stats.get('recent_avg_time', stats.get('avg_response_time', 0)):.2f}ì´ˆ",
                "ì‚¬ìš© íšŸìˆ˜": stats.get('usage_count', 0)
            })
        
        df_mcp = pd.DataFrame(mcp_data)
        st.dataframe(df_mcp, use_container_width=True)
    
    # ìµœì í™” ì´ë ¥
    st.subheader("âš¡ ìµœì í™” ì´ë ¥")
    
    opt_history = call_backend("/feedback/optimization-history")
    
    if opt_history and any(opt_history.values()):
        for tool_name, history in opt_history.items():
            if history:
                st.write(f"**{tool_name} ìµœì í™” ì´ë ¥:**")
                for opt in history[-3:]:  # ìµœê·¼ 3ê°œë§Œ í‘œì‹œ
                    st.write(f"- {opt.get('timestamp', 'N/A')}: {', '.join(opt.get('optimizations', []))}")

# === ì‹œìŠ¤í…œ ìƒíƒœ ===
elif selected == "ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ":
    st.header("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§")
    
    # í—¬ìŠ¤ì²´í¬
    health = call_backend("/health")
    
    if health:
        st.subheader("ğŸ’š ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            status_color = "ğŸŸ¢" if health["status"] == "healthy" else "ğŸ”´"
            st.write(f"**ì „ì²´ ìƒíƒœ:** {status_color} {health['status'].upper()}")
            st.write(f"**ë²„ì „:** {health['version']}")
            st.write(f"**ë§ˆì§€ë§‰ í™•ì¸:** {datetime.fromtimestamp(health['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}")
        
        with col2:
            st.write("**ì„œë¹„ìŠ¤ ìƒíƒœ:**")
            services = health.get("services", {})
            for service_name, status in services.items():
                status_icon = "âœ…" if status else "âŒ"
                st.write(f"{status_icon} {service_name}")
    
    # ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ
    st.subheader("ğŸ‘¤ ì‚¬ìš©ì ì„ í˜¸ë„")
    
    preferences = call_backend(f"/user/{st.session_state.user_id}/preferences")
    
    if preferences:
        if preferences:
            st.write("**í˜„ì¬ ì‚¬ìš©ì ì„ í˜¸ë„:**")
            for key, value in preferences.items():
                st.write(f"- **{key}:** {value}")
        else:
            st.info("ì•„ì§ í•™ìŠµëœ ì„ í˜¸ë„ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    st.subheader("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    
    with st.expander("ê³ ê¸‰ ì„¤ì •"):
        # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ì‹œë®¬ë ˆì´ì…˜)
        log_level = st.selectbox("ë¡œê·¸ ë ˆë²¨", ["DEBUG", "INFO", "WARNING", "ERROR"])
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™œì„±í™”
        enable_metrics = st.checkbox("ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘", value=True)
        
        # ìë™ ìµœì í™” í™œì„±í™”
        enable_auto_opt = st.checkbox("ìë™ ìµœì í™”", value=True)
        
        if st.button("ì„¤ì • ì ìš©"):
            st.success("ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    st.subheader("ğŸ§ª ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸")
    
    test_col1, test_col2 = st.columns(2)
    
    with test_col1:
        if st.button("ğŸ” ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"):
            with st.spinner("ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘..."):
                # ê° ë©”ëª¨ë¦¬ ìœ í˜• í…ŒìŠ¤íŠ¸
                test_results = []
                
                # ì‘ì—… ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
                try:
                    response = send_chat_message("ì‘ì—… ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸: í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì €ì¥", "basic")
                    test_results.append("âœ… ì‘ì—… ë©”ëª¨ë¦¬: ì •ìƒ")
                except:
                    test_results.append("âŒ ì‘ì—… ë©”ëª¨ë¦¬: ì˜¤ë¥˜")
                
                # ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
                try:
                    send_feedback("response_quality", "ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ìš© í”¼ë“œë°±", 4.0)
                    test_results.append("âœ… ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬: ì •ìƒ")
                except:
                    test_results.append("âŒ ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬: ì˜¤ë¥˜")
                
                for result in test_results:
                    st.write(result)
    
    with test_col2:
        if st.button("âš¡ í”¼ë“œë°± ë£¨í”„ í…ŒìŠ¤íŠ¸"):
            with st.spinner("í”¼ë“œë°± ë£¨í”„ í…ŒìŠ¤íŠ¸ ì¤‘..."):
                # 5ì´ˆ ì´ë‚´ ëª©í‘œ í…ŒìŠ¤íŠ¸
                start_time = time.time()
                response = send_feedback("tool_performance", "í”¼ë“œë°± ë£¨í”„ ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸")
                end_time = time.time()
                
                response_time = end_time - start_time
                
                if response_time < 5.0:
                    st.success(f"âœ… í”¼ë“œë°± ë£¨í”„ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ({response_time:.2f}ì´ˆ)")
                else:
                    st.error(f"âŒ í”¼ë“œë°± ë£¨í”„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ({response_time:.2f}ì´ˆ > 5ì´ˆ)")
    
    # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
    st.subheader("ğŸ“¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
    
    if st.button("ì„±ëŠ¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"):
        if st.session_state.performance_data:
            df_export = pd.DataFrame(st.session_state.performance_data)
            csv = df_export.to_csv(index=False)
            st.download_button(
                label="CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=csv,
                file_name=f"performance_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        else:
            st.info("ë‚´ë³´ë‚¼ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# === í•˜ë‹¨ ìƒíƒœ í‘œì‹œì¤„ ===
st.markdown("---")
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.write(f"**ì„¸ì…˜ ID:** {st.session_state.session_id[-8:]}")

with col2:
    st.write(f"**ì‚¬ìš©ì:** {st.session_state.user_id}")

with col3:
    st.write(f"**ë©”ì‹œì§€ ìˆ˜:** {len(st.session_state.messages)}")

with col4:
    if st.session_state.performance_data:
        last_response_time = st.session_state.performance_data[-1]["processing_time"]
        st.write(f"**ìµœê·¼ ì‘ë‹µ:** {last_response_time:.2f}ì´ˆ")
    else:
        st.write("**ìµœê·¼ ì‘ë‹µ:** N/A")

# === evaluation/poc_evaluator.py ===
import asyncio
import time
import json
import requests
from typing import Dict, List, Any
from datetime import datetime
import pandas as pd
import logging

class EnhancedPoCEvaluator:
    def __init__(self, backend_url: str = "http://localhost:8000"):
        self.backend_url = backend_url
        self.test_results = {}
        self.performance_data = []
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def call_api(self, endpoint: str, data: Dict = None) -> Dict:
        """API í˜¸ì¶œ with timeout and retry"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if data:
                    response = requests.post(f"{self.backend_url}{endpoint}", 
                                           json=data, timeout=30)
                else:
                    response = requests.get(f"{self.backend_url}{endpoint}", 
                                          timeout=30)
                
                if response.status_code == 200:
                    return response.json()
                else:
                    self.logger.error(f"HTTP {response.status_code}: {response.text}")
                    return {"error": f"HTTP {response.status_code}"}
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    return {"error": str(e)}
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return {"error": "Max retries exceeded"}
    
    def test_procedural_memory_flow_mode(self):
        """ì ˆì°¨ì  ë©”ëª¨ë¦¬ - í”Œë¡œìš° ëª¨ë“œ ê²€ì¦"""
        self.logger.info("ğŸ§  ì ˆì°¨ì  ë©”ëª¨ë¦¬ (í”Œë¡œìš° ëª¨ë“œ) í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        test_case = {
            "message": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ SHE ë¹„ìƒ ì •ë³´ ì¡°íšŒí•˜ê³  Slackìœ¼ë¡œ ì•Œë¦¼í•´ì£¼ì„¸ìš”",
            "user_id": "test_procedural_user",
            "mode": "flow",
            "expected_tools": ["SEARCH_DB", "GENERATE_MSG", "SEND_SLACK"]
        }
        
        results = []
        
        # 3ë²ˆ ë°˜ë³µ ì‹¤í–‰ìœ¼ë¡œ íŒ¨í„´ í•™ìŠµ í™•ì¸
        for iteration in range(3):
            session_id = f"procedural_flow_test_{iteration}"
            
            start_time = time.time()
            response = self.call_api("/chat", {
                "message": test_case["message"],
                "user_id": test_case["user_id"],
                "session_id": session_id,
                "mode": test_case["mode"]
            })
            end_time = time.time()
            
            if "error" not in response:
                used_tools = [tool["tool_type"] for tool in response.get("tools_used", [])]
                workflow_executed = response.get("workflow_executed")
                
                result = {
                    "iteration": iteration + 1,
                    "processing_time": end_time - start_time,
                    "tools_used": used_tools,
                    "workflow_pattern": workflow_executed is not None,
                    "success": len(used_tools) >= 2,  # ìµœì†Œ 2ê°œ ë„êµ¬ ì‚¬ìš©
                    "pattern_reuse": iteration > 0 and workflow_executed is not None
                }
                
                results.append(result)
                self.logger.info(f"  ë°˜ë³µ {iteration + 1}: {result['processing_time']:.2f}ì´ˆ - {'âœ…' if result['success'] else 'âŒ'}")
            else:
                results.append({
                    "iteration": iteration + 1,
                    "error": response["error"],
                    "success": False
                })
        
        # ì„±ëŠ¥ ê°œì„  í™•ì¸ (ì‹œê°„ ë‹¨ì¶•)
        if len(results) >= 3:
            time_improvement = results[0]["processing_time"] - results[-1]["processing_time"]
            pattern_learning = sum(1 for r in results[1:] if r.get("pattern_reuse", False))
            
            success_metrics = {
                "total_tests": len(results),
                "successful_tests": sum(1 for r in results if r.get("success", False)),
                "success_rate": sum(1 for r in results if r.get("success", False)) / len(results),
                "time_improvement": time_improvement,
                "pattern_learning_rate": pattern_learning / (len(results) - 1) if len(results) > 1 else 0,
                "avg_processing_time": sum(r.get("processing_time", 0) for r in results) / len(results)
            }
            
            return {
                "test_name": "ì ˆì°¨ì  ë©”ëª¨ë¦¬ - í”Œë¡œìš° ëª¨ë“œ",
                "results": results,
                "metrics": success_metrics,
                "passed": success_metrics["success_rate"] >= 0.8 and success_metrics["pattern_learning_rate"] >= 0.5
            }
        
        return {"test_name": "ì ˆì°¨ì  ë©”ëª¨ë¦¬ - í”Œë¡œìš° ëª¨ë“œ", "results": results, "passed": False}
    
    def test_episodic_memory_personalization(self):
        """ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ - ê°œì¸í™” í•™ìŠµ ê²€ì¦"""
        self.logger.info("ğŸ“š ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ ê°œì¸í™” í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        user_id = "test_episodic_user"
        
        # 1ë‹¨ê³„: ì„ í˜¸ë„ í•™ìŠµ
        preference_session = f"episodic_pref_{int(time.time())}"
        preference_response = self.call_api("/chat", {
            "message": "ì €ëŠ” ê¸°ìˆ ì ì¸ ì„¤ëª…ì„ ì„ í˜¸í•˜ê³ , ê°„ê²°í•œ ë©”ì‹œì§€ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤",
            "user_id": user_id,
            "session_id": preference_session,
            "mode": "basic"
        })
        
        # í”¼ë“œë°±ìœ¼ë¡œ ì„ í˜¸ë„ ê°•í™”
        feedback_response = self.call_api("/feedback", {
            "session_id": preference_session,
            "user_id": user_id,
            "feedback_type": "style_preference",
            "content": "ê¸°ìˆ ì ì´ê³  ê°„ê²°í•œ ìŠ¤íƒ€ì¼ì„ ì„ í˜¸í•©ë‹ˆë‹¤",
            "rating": 5.0
        })
        
        time.sleep(2)  # ë©”ëª¨ë¦¬ ì²˜ë¦¬ ì‹œê°„ ëŒ€ê¸°
        
        # 2ë‹¨ê³„: ìƒˆ ì„¸ì…˜ì—ì„œ ê°œì¸í™” ì ìš© í™•ì¸
        test_session = f"episodic_test_{int(time.time())}"
        test_response = self.call_api("/chat", {
            "message": "ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "user_id": user_id,
            "session_id": test_session,
            "mode": "basic"
        })
        
        # 3ë‹¨ê³„: ì„ í˜¸ë„ ì¡°íšŒë¡œ í•™ìŠµ í™•ì¸
        preferences_response = self.call_api(f"/user/{user_id}/preferences")
        
        results = {
            "preference_learning": "error" not in preference_response,
            "feedback_processing": "error" not in feedback_response and feedback_response.get("applied", False),
            "personalization_applied": "error" not in test_response and len(test_response.get("memory_used", {}).get("EPISODIC", [])) > 0,
            "preferences_stored": "error" not in preferences_response and len(preferences_response) > 0,
            "processing_times": {
                "preference": preference_response.get("processing_time", 0),
                "feedback": feedback_response.get("processing_time", 0),
                "personalized": test_response.get("processing_time", 0)
            }
        }
        
        success_rate = sum(1 for k, v in results.items() if k != "processing_times" and v) / 4
        
        return {
            "test_name": "ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ - ê°œì¸í™” í•™ìŠµ",
            "results": results,
            "metrics": {
                "success_rate": success_rate,
                "preference_recall": results["personalization_applied"],
                "feedback_responsiveness": results["feedback_processing"]
            },
            "passed": success_rate >= 0.75
        }
    
    def test_5_second_feedback_target(self):
        """5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬ ëª©í‘œ ê²€ì¦"""
        self.logger.info("âš¡ 5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        feedback_tests = [
            {"type": "style_preference", "content": "ë” ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”"},
            {"type": "response_quality", "content": "ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ìš”", "rating": 2.0},
            {"type": "tool_performance", "content": "ë°ì´í„° ì¡°íšŒê°€ ëŠë ¤ìš”"},
            {"type": "workflow_efficiency", "content": "ë‹¨ê³„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”"},
            {"type": "user_experience", "content": "ë” ì§ê´€ì ì´ì—ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”"}
        ]
        
        results = []
        
        for i, test in enumerate(feedback_tests):
            session_id = f"feedback_speed_test_{i}"
            
            start_time = time.time()
            response = self.call_api("/feedback", {
                "session_id": session_id,
                "user_id": "feedback_speed_tester",
                "feedback_type": test["type"],
                "content": test["content"],
                "rating": test.get("rating")
            })
            end_time = time.time()
            
            processing_time = end_time - start_time
            meets_target = processing_time < 5.0
            
            result = {
                "test_case": i + 1,
                "feedback_type": test["type"],
                "processing_time": processing_time,
                "meets_5s_target": meets_target,
                "applied": response.get("applied", False),
                "optimizations_count": len(response.get("optimizations", []))
            }
            
            results.append(result)
            
            status = "âœ…" if meets_target else "âŒ"
            self.logger.info(f"  í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.3f}ì´ˆ - {status}")
        
        # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        target_achievement_rate = sum(1 for r in results if r["meets_5s_target"]) / len(results)
        avg_processing_time = sum(r["processing_time"] for r in results) / len(results)
        application_rate = sum(1 for r in results if r["applied"]) / len(results)
        
        return {
            "test_name": "5ì´ˆ ì´ë‚´ í”¼ë“œë°± ì²˜ë¦¬",
            "results": results,
            "metrics": {
                "target_achievement_rate": target_achievement_rate,
                "avg_processing_time": avg_processing_time,
                "application_rate": application_rate,
                "total_tests": len(results)
            },
            "passed": target_achievement_rate >= 0.95  # 95% ëª©í‘œ
        }
    
    def test_cross_agent_learning(self):
        """í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ê²€ì¦"""
        self.logger.info("ğŸ¤ í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        user_id = "cross_agent_test_user"
        
        # Agent 1ì—ì„œ í•™ìŠµ
        agent1_session = f"cross_agent_1_{int(time.time())}"
        
        # ì„ í˜¸ë„ ì„¤ì •
        learning_response = self.call_api("/chat", {
            "message": "ì €ëŠ” í•­ìƒ Slack ì•Œë¦¼ì„ ì„ í˜¸í•˜ê³ , ìƒì„¸í•œ ë¡œê·¸ ì •ë³´ë¥¼ ì›í•©ë‹ˆë‹¤",
            "user_id": user_id,
            "session_id": agent1_session,
            "mode": "basic"
        })
        
        # í”¼ë“œë°±ìœ¼ë¡œ í•™ìŠµ ê°•í™”
        feedback_response = self.call_api("/feedback", {
            "session_id": agent1_session,
            "user_id": user_id,
            "feedback_type": "style_preference",
            "content": "ìƒì„¸í•œ ê¸°ìˆ  ì •ë³´ë¥¼ í¬í•¨í•´ì„œ Slackìœ¼ë¡œ ì•Œë¦¼í•´ì£¼ì„¸ìš”",
            "rating": 5.0
        })
        
        time.sleep(3)  # í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ì „íŒŒ ì‹œê°„
        
        # Agent 2ì—ì„œ í•™ìŠµ ë‚´ìš© í™œìš© í™•ì¸
        agent2_session = f"cross_agent_2_{int(time.time())}"
        
        application_response = self.call_api("/chat", {
            "message": "ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
            "user_id": user_id,  # ë™ì¼í•œ ì‚¬ìš©ì
            "session_id": agent2_session,  # ë‹¤ë¥¸ ì„¸ì…˜ (ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
            "mode": "basic"
        })
        
        # ì„ í˜¸ë„ ë°ì´í„° í™•ì¸
        preferences_response = self.call_api(f"/user/{user_id}/preferences")
        
        results = {
            "initial_learning": "error" not in learning_response,
            "feedback_applied": feedback_response.get("applied", False),
            "cross_agent_memory_used": len(application_response.get("memory_used", {}).get("EPISODIC", [])) > 0,
            "preferences_shared": len(preferences_response) > 0,
            "processing_times": {
                "learning": learning_response.get("processing_time", 0),
                "feedback": feedback_response.get("processing_time", 0),
                "application": application_response.get("processing_time", 0)
            },
            "tools_optimization": any("slack" in tool.get("tool_type", "").lower() 
                                    for tool in application_response.get("tools_used", []))
        }
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_indicators = ["initial_learning", "feedback_applied", "cross_agent_memory_used", "preferences_shared"]
        success_rate = sum(1 for indicator in success_indicators if results[indicator]) / len(success_indicators)
        
        return {
            "test_name": "í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ",
            "results": results,
            "metrics": {
                "success_rate": success_rate,
                "learning_transfer_success": results["cross_agent_memory_used"],
                "preference_sharing_success": results["preferences_shared"]
            },
            "passed": success_rate >= 0.8  # 80% ëª©í‘œ
        }
    
    def test_mcp_tool_performance_optimization(self):
        """MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™” ê²€ì¦"""
        self.logger.info("ğŸ”§ MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ì´ˆê¸° ì„±ëŠ¥ ì¸¡ì •
        initial_stats = self.call_api("/mcp/tools/performance")
        
        # ì—¬ëŸ¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        test_workflows = [
            {
                "message": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ ì¡°íšŒí•˜ê³  ê²°ê³¼ë¥¼ Slackìœ¼ë¡œ ì „ì†¡í•´ì£¼ì„¸ìš”",
                "expected_tools": ["SEARCH_DB", "SEND_SLACK"]
            },
            {
                "message": "ë¹„ìƒ ìƒí™© ì•Œë¦¼ì„ ìƒì„±í•˜ê³  ì´ë©”ì¼ê³¼ Slackìœ¼ë¡œ ë™ì‹œ ë°œì†¡í•´ì£¼ì„¸ìš”", 
                "expected_tools": ["EMERGENCY_MAIL", "SEND_SLACK"]
            },
            {
                "message": "ì‹œìŠ¤í…œ ìƒíƒœ ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì„œ ê´€ë¦¬ìì—ê²Œ ì „ë‹¬í•´ì£¼ì„¸ìš”",
                "expected_tools": ["GENERATE_MSG", "SEND_SLACK"]
            }
        ]
        
        performance_results = []
        
        for i, workflow in enumerate(test_workflows):
            session_id = f"mcp_perf_test_{i}"
            
            start_time = time.time()
            response = self.call_api("/chat", {
                "message": workflow["message"],
                "user_id": "mcp_performance_tester",
                "session_id": session_id,
                "mode": "flow"
            })
            end_time = time.time()
            
            if "error" not in response:
                tools_used = response.get("tools_used", [])
                successful_tools = [t for t in tools_used if t.get("success")]
                
                result = {
                    "workflow": i + 1,
                    "total_time": end_time - start_time,
                    "tools_used": len(tools_used),
                    "successful_tools": len(successful_tools),
                    "success_rate": len(successful_tools) / len(tools_used) if tools_used else 0,
                    "avg_tool_time": sum(t.get("execution_time", 0) for t in tools_used) / len(tools_used) if tools_used else 0
                }
                
                performance_results.append(result)
                self.logger.info(f"  ì›Œí¬í”Œë¡œìš° {i+1}: {result['total_time']:.2f}ì´ˆ, ì„±ê³µë¥  {result['success_rate']:.1%}")
        
        # ìµœì¢… ì„±ëŠ¥ ì¸¡ì •
        final_stats = self.call_api("/mcp/tools/performance")
        
        # ì„±ëŠ¥ ê°œì„  í™•ì¸
        improvements = {}
        if initial_stats and final_stats:
            for tool_name in initial_stats.keys():
                if tool_name in final_stats:
                    initial_time = initial_stats[tool_name].get("recent_avg_time", 
                                                              initial_stats[tool_name].get("avg_response_time", 0))
                    final_time = final_stats[tool_name].get("recent_avg_time",
                                                          final_stats[tool_name].get("avg_response_time", 0))
                    
                    if initial_time > 0:
                        improvements[tool_name] = {
                            "time_improvement": initial_time - final_time,
                            "improvement_rate": (initial_time - final_time) / initial_time if initial_time > 0 else 0
                        }
        
        metrics = {
            "total_workflows": len(performance_results),
            "avg_success_rate": sum(r["success_rate"] for r in performance_results) / len(performance_results) if performance_results else 0,
            "avg_processing_time": sum(r["total_time"] for r in performance_results) / len(performance_results) if performance_results else 0,
            "performance_improvements": improvements
        }
        
        return {
            "test_name": "MCP ë„êµ¬ ì„±ëŠ¥ ìµœì í™”",
            "results": performance_results,
            "metrics": metrics,
            "passed": metrics["avg_success_rate"] >= 0.85 and metrics["avg_processing_time"] <= 5.0
        }
    
    def run_comprehensive_evaluation(self):
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        self.logger.info("ğŸš€ Enhanced Agentic AI PoC ì¢…í•© í‰ê°€ ì‹œì‘...")
        self.logger.info("=" * 60)
        
        evaluation_results@app.get("/feedback/optimization-history")
async def optimization_history_endpoint():
    """ìµœì í™” ì´ë ¥ ì¡°íšŒ"""
    try:
        history = await feedback_service.get_optimization_history()
        return history
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìµœì í™” ì´ë ¥ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@app.get("/user/{user_id}/preferences")
async def user_preferences_endpoint(user_id: str):
    """ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ"""
    try:
        preferences = await feedback_service.get_user_preferences(user_id)
        return preferences
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„ í˜¸ë„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "services": {
            "memory": memory_service is not None,
            "mcp": mcp_service is not None,
            "feedback": feedback_service is not None,
            "agent": agent_service is not None
        },
        "version": "2.0.0"
    }

@app.get("/metrics")
async def metrics():
    """Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

# === frontend/Dockerfile ===
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8501

HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health

CMD ["streamlit", "run", "streamlit_app.py", "--server.address", "0.0.0.0", "--server.port", "8501"]

# === frontend/requirements.txt ===
streamlit==1.29.0
requests==2.31.0
plotly==5.17.0
pandas==2.1.4
altair==5.2.0
streamlit-option-menu==0.3.6
streamlit-autorefresh==0.0.1

# === frontend/streamlit_app.py ===
import streamlit as st
import requests
import time
import json
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
from datetime import datetime, timedelta
import asyncio
from streamlit_option_menu import option_menu
from streamlit_autorefresh import st_autorefresh

# ì„¤ì •
BACKEND_URL = "http://backend:8000"

st.set_page_config(
    page_title="Enhanced Agentic AI PoC",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "session_id" not in st.session_state:
    st.session_state.session_id = f"session_{int(time.time())}"
if "user_id" not in st.session_state:
    st.session_state.user_id = "demo_user"
if "performance_data" not in st.session_state:
    st.session_state.performance_data = []
if "feedback_history" not in st.session_state:
    st.session_state.feedback_history = []

def call_backend(endpoint, method="GET", data=None):
    """ë°±ì—”ë“œ API í˜¸ì¶œ"""
    try:
        url = f"{BACKEND_URL}{endpoint}"
        if method == "GET":
            response = requests.get(url, timeout=30)
        else:
            response = requests.post(url, json=data, timeout=30)
        
        if response.status_code == 200:
            return response.json()
        else:
            st.error(f"ë°±ì—”ë“œ ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        st.error(f"ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        return None

def send_chat_message(message, mode):
    """ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡"""
    data = {
        "message": message,
        "user_id": st.session_state.user_id,
        "session_id": st.session_state.session_id,
        "mode": mode,
        "context": {}
    }
    
    with st.spinner("ğŸ¤– AIê°€ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
        response = call_backend("/chat", method="POST", data=data)
    
    if response:
        # ì„±ëŠ¥ ë°ì´í„° ì €ì¥
        st.session_state.performance_data.append({
            "timestamp": datetime.now(),
            "processing_time": response.get("processing_time", 0),
            "tools_used": len(response.get("tools_used", [])),
            "confidence_score": response.get("confidence_score", 0),
            "mode": mode,
            "memory_types": len([k for k, v in response.get("memory_used", {}).items() if v])
        })
        
        return response
    return None

def send_feedback(feedback_type, content, rating=None):
    """í”¼ë“œë°± ì „ì†¡"""
    data = {
        "session_id": st.session_state.session_id,
        "user_id": st.session_state.user_id,
        "feedback_type": feedback_type,
        "content": content,
        "rating": rating
    }
    
    start_time = time.time()
    response = call_backend("/feedback", method="POST", data=data)
    end_time = time.time()
    
    if response:
        feedback_record = {
            "timestamp": datetime.now(),
            "type": feedback_type,
            "content": content,
            "response_time": end_time - start_time,
            "optimizations": response.get("optimizations", []),
            "applied": response.get("applied", False)
        }
        st.session_state.feedback_history.append(feedback_record)
        
        # 5ì´ˆ ì´ë‚´ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í‘œì‹œ
        if end_time - start_time < 5.0:
            st.success(f"âœ… í”¼ë“œë°±ì´ {end_time - start_time:.2f}ì´ˆ ë§Œì— ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.warning(f"âš ï¸ í”¼ë“œë°± ì²˜ë¦¬ê°€ {end_time - start_time:.2f}ì´ˆ ì†Œìš”ë˜ì—ˆìŠµë‹ˆë‹¤ (ëª©í‘œ: 5ì´ˆ ì´ë‚´)")
        
        if response.get("optimizations"):
            st.write("**ì ìš©ëœ ìµœì í™”:**")
            for opt in response["optimizations"]:
                st.write(f"- {opt}")
    
    return response

# === ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ===
st.title("ğŸ¤– Enhanced Agentic AI PoC")
st.markdown("**MCP ë„êµ¬ ê¸°ë°˜ ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ì™€ 5ì´ˆ ì´ë‚´ í”¼ë“œë°± ë£¨í”„**")

# ì‚¬ì´ë“œë°” ë©”ë‰´
with st.sidebar:
    selected = option_menu(
        "ë©”ì¸ ë©”ë‰´",
        ["ğŸ’¬ ì±„íŒ…", "ğŸ§  ë©”ëª¨ë¦¬ ë¶„ì„", "âš¡ í”¼ë“œë°± ì„¼í„°", "ğŸ“Š ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ", "ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ"],
        icons=['chat', 'brain', 'lightning', 'graph-up', 'gear'],
        menu_icon="robot",
        default_index=0,
    )
    
    st.markdown("---")
    
    # ì‚¬ìš©ì ì„¤ì •
    st.header("ğŸ‘¤ ì‚¬ìš©ì ì„¤ì •")
    st.session_state.user_id = st.text_input("ì‚¬ìš©ì ID", value=st.session_state.user_id)
    
    # ìƒˆ ì„¸ì…˜ ì‹œì‘
    if st.button("ğŸ”„ ìƒˆ ì„¸ì…˜ ì‹œì‘"):
        st.session_state.session_id = f"session_{int(time.time())}"
        st.session_state.messages = []
        st.success("ìƒˆ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.rerun()

# === ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ===
if selected == "ğŸ’¬ ì±„íŒ…":
    st.header("ğŸ’¬ ì§€ëŠ¥í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤")
    
    # ëª¨ë“œ ì„ íƒ
    col1, col2 = st.columns(2)
    with col1:
        mode = st.radio("ì‹¤í–‰ ëª¨ë“œ", ["flow", "basic"], 
                       help="Flow: êµ¬ì¡°í™”ëœ Step-Action-Tool, Basic: ììœ¨ì  ë„êµ¬ ì„ íƒ")
    with col2:
        st.write("**í˜„ì¬ ì„¸ì…˜:**", st.session_state.session_id[-8:])
        st.write("**ì²˜ë¦¬ëœ ë©”ì‹œì§€:**", len(st.session_state.messages) // 2)
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ë²„íŠ¼ë“¤
    st.subheader("ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
    test_col1, test_col2, test_col3 = st.columns(3)
    
    with test_col1:
        if st.button("ğŸ“Š ë°ì´í„° ì¡°íšŒ í…ŒìŠ¤íŠ¸"):
            test_message = "ì‚¬ìš©ì ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  Slackìœ¼ë¡œ ì•Œë¦¼í•´ì£¼ì„¸ìš”"
            st.session_state.test_message = test_message
    
    with test_col2:
        if st.button("ğŸš¨ ë¹„ìƒ ì•Œë¦¼ í…ŒìŠ¤íŠ¸"):
            test_message = "SHE ë¹„ìƒ ìƒí™©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê¸´ê¸‰ ì•Œë¦¼ì„ ë³´ë‚´ì£¼ì„¸ìš”"
            st.session_state.test_message = test_message
    
    with test_col3:
        if st.button("ğŸ”„ ì ˆì°¨ í•™ìŠµ í…ŒìŠ¤íŠ¸"):
            test_message = "ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  ê´€ë ¨ íŒ€ì— ì „ì†¡í•´ì£¼ì„¸ìš”"
            st.session_state.test_message = test_message
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                
                if message["role"] == "assistant" and "metadata" in message:
                    with st.expander("ğŸ“Š ì‹¤í–‰ ìƒì„¸ ì •ë³´"):
                        metadata = message["metadata"]
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ì²˜ë¦¬ ì‹œê°„", f"{metadata.get('processing_time', 0):.2f}ì´ˆ")
                        with col2:
                            st.metric("ì‚¬ìš© ë„êµ¬", f"{len(metadata.get('tools_used', []))}ê°œ")
                        with col3:
                            st.metric("ì‹ ë¢°ë„", f"{metadata.get('confidence_score', 0):.1%}")
                        
                        if metadata.get("workflow_executed"):
                            st.write("**ì‹¤í–‰ëœ ì›Œí¬í”Œë¡œìš°:**", metadata["workflow_executed"].get("pattern_name", "N/A"))
                        
                        if metadata.get("memory_used"):
                            st.write("**í™œìš©ëœ ë©”ëª¨ë¦¬:**")
                            for memory_type, items in metadata["memory_used"].items():
                                if items:
                                    st.write(f"- {memory_type}: {len(items)}ê°œ í•­ëª©")
                        
                        if metadata.get("optimization_applied"):
                            st.write("**ì ìš©ëœ ìµœì í™”:**")
                            for opt in metadata["optimization_applied"]:
                                st.write(f"- {opt}")
    
    # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ìë™ ì…ë ¥
    default_message = ""
    if hasattr(st.session_state, 'test_message'):
        default_message = st.session_state.test_message
        delattr(st.session_state, 'test_message')
    
    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", value=default_message):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ì‘ë‹µ ì²˜ë¦¬
        response = send_chat_message(prompt, mode)
        
        if response:
            # AI ì‘ë‹µ ì¶”ê°€
            assistant_message = {
                "role": "assistant",
                "content": response["response"],
                "metadata": response
            }
            st.session_state.messages.append(assistant_message)
            
            with st.chat_message("assistant"):
                st.markdown(response["response"])
                
                with st.expander("ğŸ“Š ì‹¤í–‰ ìƒì„¸ ì •ë³´"):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{response.get('processing_time', 0):.2f}ì´ˆ")
                    with col2:
                        st.metric("ì‚¬ìš© ë„êµ¬", f"{len(response.get('tools_used', []))}ê°œ")
                    with col3:
                        st.metric("ì‹ ë¢°ë„", f"{response.get('confidence_score', 0):.1%}")
    
    # ì¦‰ì‹œ í”¼ë“œë°± ì„¹ì…˜
    with st.expander("âš¡ ì¦‰ì‹œ í”¼ë“œë°± (5ì´ˆ ì´ë‚´ ëª©í‘œ)", expanded=False):
        feedback_col1, feedback_col2 = st.columns(2)
        
        with feedback_col1:
            st.write("**ìŠ¤íƒ€ì¼ í”¼ë“œë°±**")
            if st.button("ğŸ­ ë” ì¹œê·¼í•˜ê²Œ"):
                send_feedback("style_preference", "ë” ì¹œê·¼í•˜ê³  ìºì£¼ì–¼í•œ í†¤ìœ¼ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”")
            if st.button("ğŸ’¼ ë” ê³µì‹ì ìœ¼ë¡œ"):
                send_feedback("style_preference", "ë” ê³µì‹ì ì´ê³  ë¹„ì¦ˆë‹ˆìŠ¤ í†¤ìœ¼ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”")
            if st.button("ğŸ”¬ ë” ê¸°ìˆ ì ìœ¼ë¡œ"):
                send_feedback("style_preference", "ë” ê¸°ìˆ ì ì´ê³  ì „ë¬¸ì ì¸ ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”")
        
        with feedback_col2:
            st.write("**ë‚´ìš© í”¼ë“œë°±**")
            if st.button("ğŸ“ ë” ìì„¸íˆ"):
                send_feedback("response_quality", "ì‘ë‹µì´ ë„ˆë¬´ ê°„ë‹¨í•´ìš”. ë” ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•´ìš”", rating=2.0)
            if st.button("ğŸ“‹ ë” ê°„ëµí•˜ê²Œ"):
                send_feedback("response_quality", "ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ìš”. ë” ê°„ëµí•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”", rating=2.5)
            if st.button("ğŸš€ ì†ë„ ìš°ì„ "):
                send_feedback("workflow_efficiency", "ì •í™•ë„ë³´ë‹¤ ë¹ ë¥¸ ì‘ë‹µì´ ë” ì¤‘ìš”í•´ìš”")

# === ë©”ëª¨ë¦¬ ë¶„ì„ ===
elif selected == "ğŸ§  ë©”ëª¨ë¦¬ ë¶„ì„":
    st.header("ğŸ§  ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ë¶„ì„")
    
    # ì‹¤ì‹œê°„ ìƒˆë¡œê³ ì¹¨
    st_autorefresh(interval=10000, key="memory_refresh")  # 10ì´ˆë§ˆë‹¤ ìƒˆë¡œê³ ì¹¨
    
    # ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ
    memory_stats = call_backend("/memory/stats")
    
    if memory_stats:
        st.subheader("ğŸ“Š ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í˜„í™©")
        
        # ë©”íŠ¸ë¦­ í‘œì‹œ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            working_count = memory_stats.get("working_memory", {}).get("total_keys", 0)
            st.metric("ì‘ì—… ë©”ëª¨ë¦¬", f"{working_count}ê°œ", help="í˜„ì¬ í™œì„± ì„¸ì…˜ì˜ ì„ì‹œ ë©”ëª¨ë¦¬")
        
        with col2:
            episodic_count = memory_stats.get("episodic_memory", {}).get("episodes_count", 0)
            st.metric("ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬", f"{episodic_count}ê°œ", help="ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ê¸°ë¡")
        
        with col3:
            semantic_nodes = memory_stats.get("semantic_memory", {}).get("nodes_count", 0)
            st.metric("ì‹œë§¨í‹± ë©”ëª¨ë¦¬", f"{semantic_nodes}ê°œ", help="ë„ë©”ì¸ ì§€ì‹ ë…¸ë“œ")
        
        with col4:
            procedures_count = memory_stats.get("episodic_memory", {}).get("procedures_count", 0)
            st.metric("ì ˆì°¨ ë©”ëª¨ë¦¬", f"{procedures_count}ê°œ", help="í•™ìŠµëœ ì›Œí¬í”Œë¡œìš° íŒ¨í„´")
        
        # ì„±ëŠ¥ ì§€í‘œ
        if "performance" in memory_stats:
            st.subheader("âš¡ ë©”ëª¨ë¦¬ ì„±ëŠ¥ ì§€í‘œ")
            perf = memory_stats["performance"]
            
            perf_col1, perf_col2 = st.columns(2)
            with perf_col1:
                avg_time = perf.get("avg_retrieval_time", 0)
                color = "green" if avg_time < 0.2 else "orange" if avg_time < 0.5 else "red"
                st.metric(
                    "í‰ê·  ê²€ìƒ‰ ì‹œê°„", 
                    f"{avg_time:.3f}ì´ˆ", 
                    help="ëª©í‘œ: 0.2ì´ˆ ì´í•˜",
                    delta=f"ëª©í‘œ ëŒ€ë¹„ {(avg_time - 0.2):.3f}ì´ˆ" if avg_time > 0.2 else "ëª©í‘œ ë‹¬ì„±!"
                )
            
            with perf_col2:
                total_ops = perf.get("total_operations", 0)
                st.metric("ì´ ì‘ì—… ìˆ˜", f"{total_ops:,}íšŒ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì‹œê°í™”
        st.subheader("ğŸ“ˆ ë©”ëª¨ë¦¬ ì‚¬ìš© ë¶„í¬")
        
        memory_data = {
            "ë©”ëª¨ë¦¬ ìœ í˜•": ["ì‘ì—… ë©”ëª¨ë¦¬", "ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬", "ì‹œë§¨í‹± ë©”ëª¨ë¦¬", "ì ˆì°¨ ë©”ëª¨ë¦¬"],
            "í•­ëª© ìˆ˜": [
                working_count,
                episodic_count, 
                semantic_nodes,
                procedures_count
            ]
        }
        
        df_memory = pd.DataFrame(memory_data)
        fig_pie = px.pie(df_memory, values="í•­ëª© ìˆ˜", names="ë©”ëª¨ë¦¬ ìœ í˜•", 
                        title="ë©”ëª¨ë¦¬ ìœ í˜•ë³„ ë¶„í¬")
        st.plotly_chart(fig_pie, use_container_width=True)
        
        # Redis ìƒì„¸ ì •ë³´
        if "working_memory" in memory_stats:
            st.subheader("ğŸ’¾ Redis ì‘ì—… ë©”ëª¨ë¦¬ ìƒì„¸")
            redis_info = memory_stats["working_memory"]
            
            detail_col1, detail_col2 = st.columns(2)
            with detail_col1:
                st.write(f"**ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:** {redis_info.get('memory_usage', 'N/A')}")
            with detail_col2:
                st.write(f"**ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸:** {redis_info.get('connected_clients', 0)}ê°œ")
    
    # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì„¹ì…˜
    st.subheader("ğŸ§ª ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    test_col1, test_col2 = st.columns(2)
    
    with test_col1:
        st.write("**ì ˆì°¨ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸**")
        if st.button("ğŸ“‹ ì›Œí¬í”Œë¡œìš° í•™ìŠµ í…ŒìŠ¤íŠ¸"):
            # ë™ì¼í•œ ì‘ì—…ì„ 3ë²ˆ ìˆ˜í–‰í•˜ì—¬ ì ˆì°¨ í•™ìŠµ í™•ì¸
            test_message = "ë°ì´í„° ì¡°íšŒ â†’ ë©”ì‹œì§€ ìƒì„± â†’ Slack ì „ì†¡ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"
            response = send_chat_message(test_message, "flow")
            if response:
                st.success("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ! 3íšŒ ë°˜ë³µ í›„ ìë™ í•™ìŠµì„ í™•ì¸í•´ë³´ì„¸ìš”.")
    
    with test_col2:
        st.write("**ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸**")
        if st.button("ğŸ¯ ê°œì¸í™” í•™ìŠµ í…ŒìŠ¤íŠ¸"):
            test_message = "ì œ ì„ í˜¸ë„ë¥¼ ê¸°ì–µí•´ì£¼ì„¸ìš”: ê°„ê²°í•œ ë©”ì‹œì§€, ê¸°ìˆ ì  ì„¤ëª… ì„ í˜¸"
            response = send_chat_message(test_message, "basic")
            if response:
                st.success("ì„ í˜¸ë„ ì €ì¥ ì™„ë£Œ! ë‹¤ìŒ ëŒ€í™”ì—ì„œ ê°œì¸í™” íš¨ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")

# === í”¼ë“œë°± ì„¼í„° ===
elif selected == "âš¡ í”¼ë“œë°± ì„¼í„°":
    st.header("âš¡ ì‹¤ì‹œê°„ í”¼ë“œë°± ì„¼í„°")
    st.markdown("**ëª©í‘œ: 5ì´ˆ ì´ë‚´ í”¼ë“œë°± ë°˜ì˜**")
    
    # í”¼ë“œë°± ì´ë ¥ í‘œì‹œ
    if st.session_state.feedback_history:
        st.subheader("ğŸ“Š í”¼ë“œë°± ì²˜ë¦¬ ì„±ëŠ¥")
        
        df_feedback = pd.DataFrame(st.session_state.feedback_history)
        
        # 5ì´ˆ ì´ë‚´ ë‹¬ì„±ë¥  ê³„ì‚°
        within_5s = len(df_feedback[df_feedback["response_time"] < 5.0])
        total_feedback = len(df_feedback)
        success_rate = (within_5s / total_feedback * 100) if total_feedback > 0 else 0
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("5ì´ˆ ì´ë‚´ ì²˜ë¦¬ìœ¨", f"{success_rate:.1f}%", 
                     help="ëª©í‘œ: 95% ì´ìƒ")
        with col2:
            avg_time = df_feedback["response_time"].mean()
            st.metric("í‰ê·  ì²˜ë¦¬ ì‹œê°„", f"{avg_time:.2f}ì´ˆ")
        with col3:
            st.metric("ì´ í”¼ë“œë°±", f"{total_feedback}ê±´")
        
        # í”¼ë“œë°± ì²˜ë¦¬ ì‹œê°„ ë¶„í¬ ì°¨íŠ¸
        fig_hist = px.histogram(df_feedback, x="response_time", 
                               title="í”¼ë“œë°± ì²˜ë¦¬ ì‹œê°„ ë¶„í¬",
                               labels={"response_time": "ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)", "count": "ë¹ˆë„"})
        fig_hist.add_vline(x=5.0, line_dash="dash", line_color="red", 
                          annotation_text="ëª©í‘œ: 5ì´ˆ")
        st.plotly_chart(fig_hist, use_container_width=True)
        
        # ìµœê·¼ í”¼ë“œë°± ì´ë ¥
        st.subheader("ğŸ“ ìµœê·¼ í”¼ë“œë°± ì´ë ¥")
        recent_feedback = df_feedback.tail(10).sort_values("timestamp", ascending=False)
        
        for _, feedback in recent_feedback.iterrows():
            with st.expander(f"ğŸ•’ {feedback['timestamp'].strftime('%H:%M:%S')} - {feedback['type']}"):
                st.write(f"**ë‚´ìš©:** {feedback['content']}")
                st.write(f"**ì²˜ë¦¬ ì‹œê°„:** {feedback['response_time']:.2f}ì´ˆ")
                st.write(f"**ì ìš© ì—¬ë¶€:** {'âœ… ì ìš©ë¨' if feedback['applied'] else 'âŒ ë¯¸ì ìš©'}")
                
                if feedback['optimizations']:
                    st.write("**ì ìš©ëœ ìµœì í™”:**")
                    for opt in feedback['optimizations']:
                        st.write(f"- {opt}")
    
    # ê³ ê¸‰ í”¼ë“œë°± ì…ë ¥
    st.subheader("ğŸ’¬ ìƒì„¸ í”¼ë“œë°± ì…ë ¥")
    
    feedback_type = st.selectbox("í”¼ë“œë°± ìœ í˜•", [
        "style_preference", "response_quality", "tool_performance", 
        "workflow_efficiency", "user_experience"
    ])
    
    feedback_content = st.text_area("í”¼ë“œë°± ë‚´ìš©", 
                                   placeholder="êµ¬ì²´ì ì¸ ê°œì„  ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”...")
    
    rating = st.slider("ë§Œì¡±ë„ í‰ê°€", 1.0, 5.0, 3.0, 0.5)
    
    if st.button("ğŸš€ í”¼ë“œë°± ì œì¶œ (5ì´ˆ ì´ë‚´ ëª©í‘œ)"):
        if feedback_content:
            send_feedback(feedback_type, feedback_content, rating)
        else:
            st.warning("í”¼ë“œë°± ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ í…ŒìŠ¤íŠ¸
    st.subheader("ğŸ¤ í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ í…ŒìŠ¤íŠ¸")
    st.write("ë‹¤ë¥¸ ì„¸ì…˜ì—ì„œ í•™ìŠµí•œ ì„ í˜¸ë„ê°€ í˜„ì¬ ì„¸ì…˜ì— ì ìš©ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    if st.button("ğŸ”„ ë‹¤ë¥¸ ì„¸ì…˜ ì„ í˜¸ë„ ë™ê¸°í™”"):
        # ë‹¤ë¥¸ ì‚¬ìš©ì IDë¡œ ì„ í˜¸ë„ ì„¤ì • í›„ í˜„ì¬ ì„¸ì…˜ì— ì ìš© í…ŒìŠ¤íŠ¸
        sync_response = send_feedback("preference_sync", "ë‹¤ë¥¸ ì—ì´ì „íŠ¸ í•™ìŠµ ë‚´ìš©ì„ í˜„ì¬ ì„¸ì…˜ì— ë™ê¸°í™”")
        if sync_response:
            st.success("í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ë™ê¸°í™” ì™„ë£Œ!")

# === ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ===
elif selected == "ğŸ“Š ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ":
    st.header("ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ")
    
    # ìë™ ìƒˆë¡œê³ ì¹¨
    st_autorefresh(interval=5000, key="dashboard_refresh")  # 5ì´ˆë§ˆë‹¤ ìƒˆë¡œê³ ì¹¨
    
    if st.session_state.performance_data:
        df_perf = pd.DataFrame(st.session_state.performance_data)
        
        # ìµœê·¼ 30ë¶„ ë°ì´í„°ë§Œ í‘œì‹œ
        current_time = datetime.now()
        df_recent = df_perf[df_perf["timestamp"] > (current_time - timedelta(minutes=30))]
        
        if not df_recent.empty:
            # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­
            st.subheader("âš¡ ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                avg_processing = df_recent["processing_time"].mean()
                st.metric("í‰ê·  ì²˜ë¦¬ ì‹œê°„", f"{avg_processing:.2f}ì´ˆ",
                         delta=f"{avg_processing - df_recent['processing_time'].iloc[0]:.2f}ì´ˆ")
            
            with col2:
                avg_confidence = df_recent["confidence_score"].mean()
                st.metric("í‰ê·  ì‹ ë¢°ë„", f"{avg_confidence:.1%}")
            
            with col3:
                avg_tools = df_recent["tools_used"].mean()
                st.metric("í‰ê·  ë„êµ¬ ì‚¬ìš©", f"{avg_tools:.1f}ê°œ")
            
            with col4:
                avg_memory = df_recent["memory_types"].mean()
                st.metric("í‰ê·  ë©”ëª¨ë¦¬ í™œìš©", f"{avg_memory:.1f}ê°œ ìœ í˜•")
            
            # ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ì¶”ì´
            st.subheader("ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ì¶”ì´")
            
            # ì²˜ë¦¬ ì‹œê°„ ì¶”ì´
            fig_time = px.line(df_recent, x="timestamp", y="processing_time", 
                              color="mode", title="ì²˜ë¦¬ ì‹œê°„ ì¶”ì´")
            fig_time.add_hline(y=3.0, line_dash="dash", line_color="orange", 
                              annotation_text="ëª©í‘œ: 3ì´ˆ ì´ë‚´")
            st.plotly_chart(fig_time, use_container_width=True)
            
            # ëª¨ë“œë³„ ì„±ëŠ¥ ë¹„êµ
            st.subheader("ğŸ”„ ëª¨ë“œë³„ ì„±ëŠ¥ ë¹„êµ")
            
            mode_comparison = df_recent.groupby("mode").agg({
                "processing_time": ["mean", "std"],
                "confidence_score": "mean",
                "tools_used": "mean"
            }).round(3)
            
            col1, col2 = st.columns(2)
            
            with col1:
                # ëª¨ë“œë³„ ì²˜ë¦¬ ì‹œê°„ ë°•ìŠ¤í”Œë¡¯
                fig_box = px.box(df_recent, x="mode", y="processing_time", 
                                title="ëª¨ë“œë³„ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬")
                st.plotly_chart(fig_box, use_container_width=True)
            
            with col2:
                # ëª¨ë“œë³„ ì‹ ë¢°ë„ ë¹„êµ
                fig_conf = px.bar(df_recent.groupby("mode")["confidence_score"].mean().reset_index(), 
                                 x="mode", y="confidence_score", 
                                 title="ëª¨ë“œë³„ í‰ê·  ì‹ ë¢°ë„")
                st.plotly_chart(fig_conf, use_container_width=True)
            
            # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í˜„í™©
            st.subheader("ğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í˜„í™©")
            
            target_col1, target_col2, target_col3 = st.columns(3)
            
            with target_col1:
                # ì²˜ë¦¬ ì‹œê°„ ëª©í‘œ (3ì´ˆ ì´ë‚´)
                under_3s = len(df_recent[df_recent["processing_time"] < 3.0])
                time_success_rate = (under_3s / len(df_recent) * 100)
                
                fig_gauge_time = go.Figure(go.Indicator(
                    mode = "gauge+number+delta",
                    value = time_success_rate,
                    domain = {'x': [0, 1], 'y': [0, 1]},
                    title = {'text': "ì²˜ë¦¬ ì‹œê°„ ëª©í‘œ ë‹¬ì„±ë¥  (%)"},
                    delta = {'reference': 90},
                    gauge = {'axis': {'range': [None, 100]},
                             'bar': {'color': "darkblue"},
                             'steps': [    def _calculate_expected_improvements(self, optimizations: List[str]) -> Dict[str, float]:
        """ìµœì í™”ë¡œ ì¸í•œ ì˜ˆìƒ ê°œì„  íš¨ê³¼ ê³„ì‚°"""
        improvements = {}
        
        for optimization in optimizations:
            if "ì†ë„" in optimization or "ìºì‹±" in optimization:
                improvements["response_time_improvement"] = improvements.get("response_time_improvement", 0) + 0.25
                
            elif "ì•ˆì •ì„±" in optimization or "ì‹ ë¢°ì„±" in optimization:
                improvements["reliability_improvement"] = improvements.get("reliability_improvement", 0) + 0.15
                
            elif "ì •í™•ë„" in optimization or "í’ˆì§ˆ" in optimization:
                improvements["accuracy_improvement"] = improvements.get("accuracy_improvement", 0) + 0.20
                
            elif "ë§Œì¡±ë„" in optimization or "ì‚¬ìš©ì" in optimization:
                improvements["user_satisfaction_improvement"] = improvements.get("user_satisfaction_improvement", 0) + 0.30
        
        return improvements
    
    async def get_optimization_history(self, tool_type: MCPToolType = None) -> Dict[str, Any]:
        """ìµœì í™” ì´ë ¥ ì¡°íšŒ"""
        if tool_type:
            return {
                str(tool_type): self.optimization_history.get(tool_type, [])
            }
        return {str(k): v for k, v in self.optimization_history.items()}
    
    async def get_user_preferences(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ"""
        # ë©”ëª¨ë¦¬ì—ì„œë„ ì¡°íšŒ (í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµìš©)
        preferences, _ = await self.memory_service.get_working_memory(f"preferences_{user_id}", "shared_preferences")
        
        # í˜„ì¬ ì„¸ì…˜ ì„ í˜¸ë„ì™€ ë³‘í•©
        current_preferences = self.user_preferences.get(user_id, {})
        
        return {**preferences.get("shared_preferences", {}), **current_preferences}

# === backend/services/agent_service.py ===
import asyncio
import time
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from openai import OpenAI
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from ..services.memory_service import EnhancedMemoryService
from ..services.mcp_service import MCPService
from ..services.feedback_service import EnhancedFeedbackService
from ..models.schemas import (
    AgentMode, ChatResponse, WorkflowStep, WorkflowPattern,
    MCPToolCall, MCPToolType, MemoryType, EpisodicMemory
)
from ..utils.config import config

class EnhancedAgentService:
    def __init__(self, memory_service: EnhancedMemoryService, mcp_service: MCPService, feedback_service: EnhancedFeedbackService):
        self.memory_service = memory_service
        self.mcp_service = mcp_service
        self.feedback_service = feedback_service
        self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.checkpointer = MemorySaver()
        
        # ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ì €ì¥ì†Œ
        self.learned_patterns = {}
        
    async def process_chat(self, message: str, user_id: str, session_id: str, mode: AgentMode, context: Dict[str, Any] = None) -> ChatResponse:
        """ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬"""
        start_time = time.time()
        context = context or {}
        
        # 1. ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰
        memories, memory_retrieval_time = await self.memory_service.retrieve_relevant_memories(
            user_id=user_id,
            query=message,
            memory_types=[MemoryType.PROCEDURAL, MemoryType.EPISODIC, MemoryType.SEMANTIC],
            limit=3
        )
        
        # 2. ì‚¬ìš©ì ì„ í˜¸ë„ ì ìš©
        user_preferences = await self.feedback_service.get_user_preferences(user_id)
        
        # 3. ëª¨ë“œë³„ ì²˜ë¦¬
        if mode == AgentMode.FLOW:
            workflow_result = await self._process_flow_mode(message, user_id, session_id, memories, user_preferences, context)
        else:
            workflow_result = await self._process_basic_mode(message, user_id, session_id, memories, user_preferences, context)
        
        # 4. ì‘ë‹µ ìƒì„±
        response_text = await self._generate_response(message, workflow_result, user_preferences)
        
        # 5. ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ì— ìƒí˜¸ì‘ìš© ì €ì¥
        episode = EpisodicMemory(
            episode_id=f"{session_id}_{int(time.time() * 1000)}",
            user_id=user_id,
            session_id=session_id,
            timestamp=datetime.now(),
            interaction_type="chat_interaction",
            context={
                "message": message,
                "mode": mode,
                "response": response_text,
                "tools_used": [result.tool_type for result in workflow_result["tools_used"]],
                "processing_time": time.time() - start_time
            },
            workflow_executed=workflow_result.get("workflow_pattern"),
            lessons_learned=workflow_result.get("lessons_learned", [])
        )
        
        await self.memory_service.store_episodic_memory(episode)
        
        # 6. ì„±ëŠ¥ í”¼ë“œë°± ì²˜ë¦¬
        for tool_result in workflow_result["tools_used"]:
            await self.feedback_service.process_performance_feedback(
                tool_type=tool_result.tool_type,
                execution_time=tool_result.execution_time,
                success=tool_result.success,
                error_type=tool_result.error
            )
        
        total_processing_time = time.time() - start_time
        
        return ChatResponse(
            response=response_text,
            session_id=session_id,
            workflow_executed=workflow_result.get("workflow_pattern"),
            memory_used={
                MemoryType.WORKING: [f"ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸: {len(context)} í•­ëª©"],
                MemoryType.EPISODIC: [m["content"][:100] for m in memories[MemoryType.EPISODIC][:2]],
                MemoryType.SEMANTIC: [m["content"][:100] for m in memories[MemoryType.SEMANTIC][:2]],
                MemoryType.PROCEDURAL: [m["content"][:100] for m in memories[MemoryType.PROCEDURAL][:2]]
            },
            processing_time=total_processing_time,
            tools_used=workflow_result["tools_used"],
            confidence_score=workflow_result.get("confidence_score", 0.8),
            optimization_applied=workflow_result.get("optimizations", [])
        )
    
    async def _process_flow_mode(self, message: str, user_id: str, session_id: str, 
                                memories: Dict[MemoryType, List], user_preferences: Dict, 
                                context: Dict[str, Any]) -> Dict[str, Any]:
        """í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬ - êµ¬ì¡°í™”ëœ Step-Action-Tool ì‹¤í–‰"""
        
        # 1. ìœ ì‚¬í•œ ì ˆì°¨ íŒ¨í„´ ê²€ìƒ‰
        similar_procedures, _ = await self.memory_service.retrieve_similar_procedures(message, user_id, limit=3)
        
        # 2. ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ê³„íš ë˜ëŠ” ê¸°ì¡´ íŒ¨í„´ ì¬ì‚¬ìš©
        if similar_procedures and similar_procedures[0].get("relevance_score", 0) > 0.8:
            # ê¸°ì¡´ íŒ¨í„´ ì¬ì‚¬ìš©
            workflow_pattern = await self._reuse_workflow_pattern(similar_procedures[0], context)
            optimization_note = ["ê¸°ì¡´ ì„±ê³µ íŒ¨í„´ ì¬ì‚¬ìš©"]
        else:
            # ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ê³„íš
            workflow_pattern = await self._plan_new_workflow(message, memories, user_preferences, context)
            optimization_note = ["ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ìƒì„±"]
        
        # 3. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        execution_results = await self._execute_workflow(workflow_pattern)
        
        # 4. ì„±ê³µí•œ íŒ¨í„´ì„ ì ˆì°¨ ë©”ëª¨ë¦¬ì— ì €ì¥
        if all(result.success for result in execution_results):
            workflow_pattern.success_rate = 1.0
            workflow_pattern.usage_count = workflow_pattern.usage_count + 1
            await self.memory_service.store_procedural_memory(workflow_pattern, user_id)
            optimization_note.append("ì„±ê³µ íŒ¨í„´ í•™ìŠµ ì™„ë£Œ")
        
        return {
            "workflow_pattern": workflow_pattern,
            "tools_used": execution_results,
            "confidence_score": 0.9 if all(r.success for r in execution_results) else 0.6,
            "optimizations": optimization_note,
            "lessons_learned": self._extract_lessons_from_execution(execution_results)
        }
    
    async def _process_basic_mode(self, message: str, user_id: str, session_id: str,
                                 memories: Dict[MemoryType, List], user_preferences: Dict,
                                 context: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ - ììœ¨ì  ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰"""
        
        # 1. ìì—°ì–´ ì˜ë„ ë¶„ì„
        intent_analysis = await self._analyze_user_intent(message, memories, user_preferences)
        
        # 2. ìµœì  ë„êµ¬ ì¡°í•© ì¶”ì²œ
        suggested_tools = self.mcp_service.suggest_optimal_tool_combination(message)
        
        # 3. ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ ê¸°ë°˜ ë„êµ¬ ì„ íƒ ê°œì„ 
        if memories[MemoryType.EPISODIC]:
            refined_tools = await self._refine_tools_from_episodes(suggested_tools, memories[MemoryType.EPISODIC])
        else:
            refined_tools = suggested_tools
        
        # 4. ë„êµ¬ ì‹¤í–‰
        tool_calls = []
        for tool_type in refined_tools[:3]:  # ìµœëŒ€ 3ê°œ ë„êµ¬
            tool_call = MCPToolCall(
                tool_type=tool_type,
                parameters=self._generate_tool_parameters(tool_type, message, intent_analysis),
                context=context
            )
            tool_calls.append(tool_call)
        
        execution_results = await self.mcp_service.execute_workflow(tool_calls)
        
        # 5. ì‹¤í–‰ ê²°ê³¼ ê¸°ë°˜ í•™ìŠµ
        lessons_learned = []
        if execution_results:
            success_rate = sum(1 for r in execution_results if r.success) / len(execution_results)
            if success_rate > 0.8:
                lessons_learned.append(f"ë„êµ¬ ì¡°í•© {[r.tool_type for r in execution_results]} ì„±ê³µë¥  ë†’ìŒ")
        
        return {
            "workflow_pattern": None,  # ê¸°ë³¸ ëª¨ë“œëŠ” ê³ ì • ì›Œí¬í”Œë¡œìš° ì—†ìŒ
            "tools_used": execution_results,
            "confidence_score": intent_analysis.get("confidence", 0.7),
            "optimizations": [f"ììœ¨ ì„ íƒ: {len(refined_tools)}ê°œ ë„êµ¬ í™œìš©"],
            "lessons_learned": lessons_learned
        }
    
    async def _plan_new_workflow(self, message: str, memories: Dict[MemoryType, List], 
                               user_preferences: Dict, context: Dict[str, Any]) -> WorkflowPattern:
        """ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ê³„íš"""
        
        # AIë¥¼ í†µí•œ ì›Œí¬í”Œë¡œìš° ê³„íš ìƒì„±
        system_prompt = """
        ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ Step-Action-Tool ì›Œí¬í”Œë¡œìš°ë¥¼ ê³„íší•˜ì„¸ìš”.
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
        - SEARCH_DB: ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
        - GENERATE_MSG: ë©”ì‹œì§€ ìƒì„±
        - SEND_SLACK: Slack ì•Œë¦¼ ì „ì†¡
        - EMERGENCY_MAIL: ë¹„ìƒ ë©”ì¼ ë°ì´í„° ìƒì„±
        - SEND_EMAIL: ì´ë©”ì¼ ì „ì†¡
        
        3-5ë‹¨ê³„ì˜ ë…¼ë¦¬ì  ìˆœì„œë¡œ ê³„íší•˜ì„¸ìš”.
        """
        
        user_prompt = f"""
        ìš”ì²­: {message}
        ì»¨í…ìŠ¤íŠ¸: {context}
        ì‚¬ìš©ì ì„ í˜¸ë„: {user_preferences}
        
        JSON í˜•íƒœë¡œ ì›Œí¬í”Œë¡œìš°ë¥¼ ê³„íší•˜ì„¸ìš”:
        {{
            "pattern_name": "ì›Œí¬í”Œë¡œìš° ì´ë¦„",
            "steps": [
                {{"step_id": 1, "step_name": "ë‹¨ê³„ëª…", "action": "ì•¡ì…˜", "tool_type": "SEARCH_DB", "parameters": {{}}}},
                ...
            ]
        }}
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1
            )
            
            plan_json = json.loads(response.choices[0].message.content)
            
            # WorkflowPattern ê°ì²´ ìƒì„±
            steps = []
            for step_data in plan_json["steps"]:
                tool_calls = [MCPToolCall(
                    tool_type=MCPToolType(step_data["tool_type"]),
                    parameters=step_data.get("parameters", {}),
                    context=context
                )]
                
                step = WorkflowStep(
                    step_id=step_data["step_id"],
                    step_name=step_data["step_name"],
                    action=step_data["action"],
                    tool_calls=tool_calls
                )
                steps.append(step)
            
            workflow_pattern = WorkflowPattern(
                pattern_id=f"wf_{int(time.time() * 1000)}",
                pattern_name=plan_json["pattern_name"],
                steps=steps,
                success_rate=0.0,  # ì´ˆê¸°ê°’
                avg_execution_time=0.0,
                usage_count=0,
                last_used=datetime.now()
            )
            
            return workflow_pattern
            
        except Exception as e:
            # ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ë°˜í™˜
            return self._create_default_workflow(message)
    
    async def _reuse_workflow_pattern(self, similar_procedure: Dict, context: Dict[str, Any]) -> WorkflowPattern:
        """ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ì¬ì‚¬ìš©"""
        pattern_id = similar_procedure.get("pattern_id", "default")
        
        # ê¸°ë³¸ íŒ¨í„´ ìƒì„± (ì‹¤ì œë¡œëŠ” ì €ì¥ëœ íŒ¨í„´ì„ ë¶ˆëŸ¬ì™€ì•¼ í•¨)
        return self._create_default_workflow("ì¬ì‚¬ìš© ì›Œí¬í”Œë¡œìš°")
    
    def _create_default_workflow(self, message: str) -> WorkflowPattern:
        """ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        steps = [
            WorkflowStep(
                step_id=1,
                step_name="ë°ì´í„° ìˆ˜ì§‘",
                action="ì •ë³´ ì¡°íšŒ",
                tool_calls=[MCPToolCall(
                    tool_type=MCPToolType.SEARCH_DB,
                    parameters={"query": message},
                    context={}
                )]
            ),
            WorkflowStep(
                step_id=2,
                step_name="ì‘ë‹µ ìƒì„±",
                action="ë©”ì‹œì§€ ì‘ì„±",
                tool_calls=[MCPToolCall(
                    tool_type=MCPToolType.GENERATE_MSG,
                    parameters={"content": "ì¡°íšŒ ê²°ê³¼", "style": "professional"},
                    context={}
                )]
            ),
            WorkflowStep(
                step_id=3,
                step_name="ê²°ê³¼ ì „ë‹¬",
                action="ì•Œë¦¼ ë°œì†¡",
                tool_calls=[MCPToolCall(
                    tool_type=MCPToolType.SEND_SLACK,
                    parameters={"message": "ê²°ê³¼ ë©”ì‹œì§€", "channel": "#general"},
                    context={}
                )]
            )
        ]
        
        return WorkflowPattern(
            pattern_id=f"default_wf_{int(time.time())}",
            pattern_name="ê¸°ë³¸ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°",
            steps=steps,
            success_rate=0.8,
            avg_execution_time=5.0,
            usage_count=1,
            last_used=datetime.now()
        )
    
    async def _execute_workflow(self, workflow_pattern: WorkflowPattern) -> List:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        results = []
        
        for step in workflow_pattern.steps:
            step.status = "running"
            step_start_time = time.time()
            
            # ë‹¨ê³„ë³„ ë„êµ¬ ì‹¤í–‰
            step_results = await self.mcp_service.execute_workflow(step.tool_calls)
            
            step.execution_time = time.time() - step_start_time
            step.result = step_results
            
            if all(r.success for r in step_results):
                step.status = "completed"
            else:
                step.status = "failed"
                break  # ì‹¤íŒ¨ ì‹œ ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨
            
            results.extend(step_results)
        
        return results
    
    async def _analyze_user_intent(self, message: str, memories: Dict[MemoryType, List], 
                                 user_preferences: Dict) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì˜ë„ ë¶„ì„"""
        
        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        memory_context = ""
        for memory_type, memory_list in memories.items():
            if memory_list:
                memory_context += f"\n{memory_type}: {memory_list[0].get('content', '')[:100]}"
        
        system_prompt = """
        ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë„ì™€ í•„ìš”í•œ ë„êµ¬ë¥¼ íŒŒì•…í•˜ì„¸ìš”.
        ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        user_prompt = f"""
        ë©”ì‹œì§€: {message}
        ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸: {memory_context}
        ì‚¬ìš©ì ì„ í˜¸ë„: {user_preferences}
        
        ë‹¤ìŒ í˜•íƒœë¡œ ë¶„ì„í•˜ì„¸ìš”:
        {{
            "primary_intent": "ì£¼ìš” ì˜ë„",
            "urgency": "low|medium|high",
            "domain": "ë„ë©”ì¸ ì˜ì—­",
            "required_tools": ["í•„ìš”í•œ ë„êµ¬ ëª©ë¡"],
            "confidence": 0.8
        }}
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            # ê¸°ë³¸ ë¶„ì„ ë°˜í™˜
            return {
                "primary_intent": "ì¼ë°˜ ë¬¸ì˜",
                "urgency": "medium",
                "domain": "general",
                "required_tools": ["SEARCH_DB", "GENERATE_MSG"],
                "confidence": 0.5
            }
    
    async def _refine_tools_from_episodes(self, suggested_tools: List[MCPToolType], 
                                        episodes: List[Dict]) -> List[MCPToolType]:
        """ì—í”¼ì†Œë“œ ë©”ëª¨ë¦¬ ê¸°ë°˜ ë„êµ¬ ì„ íƒ ê°œì„ """
        refined_tools = suggested_tools.copy()
        
        # ê³¼ê±° ì„±ê³µ ê²½í—˜ì—ì„œ íš¨ê³¼ì ì´ì—ˆë˜ ë„êµ¬ ìš°ì„ ìˆœìœ„ ì¡°ì •
        for episode in episodes:
            if "ì„±ê³µ" in episode.get("content", ""):
                # ì„±ê³µí•œ ì—í”¼ì†Œë“œì—ì„œ ì–¸ê¸‰ëœ ë„êµ¬ë“¤ì˜ ìš°ì„ ìˆœìœ„ ì¦ê°€
                # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ë¶„ì„ í•„ìš”
                pass
        
        return refined_tools
    
    def _generate_tool_parameters(self, tool_type: MCPToolType, message: str, 
                                intent_analysis: Dict) -> Dict[str, Any]:
        """ë„êµ¬ë³„ íŒŒë¼ë¯¸í„° ìƒì„±"""
        
        if tool_type == MCPToolType.SEARCH_DB:
            return {
                "query": message[:100],  # ì¿¼ë¦¬ ê¸¸ì´ ì œí•œ
                "table": "main_data",
                "filters": {}
            }
        
        elif tool_type == MCPToolType.GENERATE_MSG:
            style = "professional"
            if intent_analysis.get("urgency") == "high":
                style = "urgent"
            return {
                "content": message,
                "style": style,
                "length": "medium"
            }
        
        elif tool_type == MCPToolType.SEND_SLACK:
            return {
                "message": f"ì•Œë¦¼: {message}",
                "channel": "#general"
            }
        
        elif tool_type == MCPToolType.EMERGENCY_MAIL:
            return {
                "type": "general",
                "severity": intent_analysis.get("urgency", "medium"),
                "description": message
            }
        
        else:
            return {}
    
    async def _generate_response(self, message: str, workflow_result: Dict, 
                               user_preferences: Dict) -> str:
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        
        # ì‚¬ìš©ì ì„ í˜¸ ìŠ¤íƒ€ì¼ ì ìš©
        style = user_preferences.get("message_style", "professional")
        
        tools_used = [r.tool_type for r in workflow_result["tools_used"] if r.success]
        success_count = sum(1 for r in workflow_result["tools_used"] if r.success)
        
        if success_count == len(workflow_result["tools_used"]):
            base_response = f"ìš”ì²­ì„ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ë„êµ¬: {', '.join(map(str, tools_used))}"
        else:
            base_response = f"ìš”ì²­ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ì„±ê³µí•œ ë„êµ¬: {success_count}/{len(workflow_result['tools_used'])}"
        
        # ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ì‘ë‹µ ì¡°ì •
        if style == "casual":
            response = f"ì•ˆë…•í•˜ì„¸ìš”! {base_response} ğŸ˜Š"
        elif style == "technical":
            response = f"[ì²˜ë¦¬ ì™„ë£Œ] {base_response}\nì‹¤í–‰ ì‹œê°„: {workflow_result.get('processing_time', 0):.2f}ì´ˆ"
        elif style == "concise":
            response = base_response
        else:  # professional
            response = f"ì•ˆë…•í•˜ì„¸ìš”,\n\n{base_response}\n\nê°ì‚¬í•©ë‹ˆë‹¤."
        
        return response
    
    def _extract_lessons_from_execution(self, execution_results: List) -> List[str]:
        """ì‹¤í–‰ ê²°ê³¼ì—ì„œ êµí›ˆ ì¶”ì¶œ"""
        lessons = []
        
        success_count = sum(1 for r in execution_results if r.success)
        total_time = sum(r.execution_time for r in execution_results)
        
        if success_count == len(execution_results):
            lessons.append("ëª¨ë“  ë„êµ¬ ì‹¤í–‰ ì„±ê³µ")
        
        if total_time > 10.0:
            lessons.append("ì‹¤í–‰ ì‹œê°„ ìµœì í™” í•„ìš”")
        
        # ì‹¤íŒ¨í•œ ë„êµ¬ê°€ ìˆëŠ” ê²½ìš°
        failed_tools = [r.tool_type for r in execution_results if not r.success]
        if failed_tools:
            lessons.append(f"ì‹¤íŒ¨ ë„êµ¬: {failed_tools} - ëŒ€ì•ˆ ë„êµ¬ ê²€í†  í•„ìš”")
        
        return lessons

# === backend/main.py ===
import asyncio
import time
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from fastapi.responses import Response

from .models.schemas import (
    ChatRequest, ChatResponse, FeedbackRequest, FeedbackResponse,
    SystemMetrics, AgentMode
)
from .services.memory_service import EnhancedMemoryService
from .services.mcp_service import MCPService
from .services.feedback_service import EnhancedFeedbackService
from .services.agent_service import EnhancedAgentService
from .utils.config import config

# Prometheus ë©”íŠ¸ë¦­
REQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')
MEMORY_OPERATIONS = Counter('memory_operations_total', 'Memory operations', ['operation_type'])

# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
memory_service = None
mcp_service = None
feedback_service = None
agent_service = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    global memory_service, mcp_service, feedback_service, agent_service
    
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    logging.info("ğŸš€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")
    
    memory_service = EnhancedMemoryService()
    await memory_service.initialize()
    logging.info("âœ… ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    mcp_service = MCPService()
    logging.info("âœ… MCP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    feedback_service = EnhancedFeedbackService(memory_service)
    logging.info("âœ… í”¼ë“œë°± ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    agent_service = EnhancedAgentService(memory_service, mcp_service, feedback_service)
    logging.info("âœ… ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    logging.info("ğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logging.info("ğŸ›‘ ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")
    if memory_service.redis_client:
        await memory_service.redis_client.close()
    if memory_service.neo4j_driver:
        memory_service.neo4j_driver.close()
    logging.info("âœ… ì •ë¦¬ ì™„ë£Œ")

app = FastAPI(
    title="Enhanced Agentic AI PoC",
    description="MCP ë„êµ¬ ê¸°ë°˜ ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ì™€ í”¼ë“œë°± ë£¨í”„ë¥¼ ê°–ì¶˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ",
    version="2.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.middleware("http")
async def add_prometheus_metrics(request, call_next):
    """Prometheus ë©”íŠ¸ë¦­ ë¯¸ë“¤ì›¨ì–´"""
    start_time = time.time()
    
    response = await call_next(request)
    
    # ë©”íŠ¸ë¦­ ê¸°ë¡
    REQUEST_COUNT.labels(method=request.method, endpoint=request.url.path).inc()
    REQUEST_DURATION.observe(time.time() - start_time)
    
    return response

# === API ì—”ë“œí¬ì¸íŠ¸ ===

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """ë©”ì¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        start_time = time.time()
        
        # ì‚¬ìš©ì ì„ í˜¸ë„ í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµ ì ìš©
        await feedback_service.process_immediate_feedback(FeedbackRequest(
            session_id=request.session_id,
            user_id=request.user_id,
            feedback_type="preference_sync",
            content="í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ ì„ í˜¸ë„ ë™ê¸°í™”"
        ))
        
        # ì±„íŒ… ì²˜ë¦¬
        response = await agent_service.process_chat(
            message=request.message,
            user_id=request.user_id,
            session_id=request.session_id,
            mode=request.mode,
            context=request.context
        )
        
        MEMORY_OPERATIONS.labels(operation_type="chat_processing").inc()
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@app.post("/feedback", response_model=FeedbackResponse)
async def feedback_endpoint(request: FeedbackRequest):
    """ì¦‰ì‹œ í”¼ë“œë°± ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        response = await feedback_service.process_immediate_feedback(request)
        MEMORY_OPERATIONS.labels(operation_type="feedback_processing").inc()
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í”¼ë“œë°± ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@app.get("/memory/stats")
async def memory_stats_endpoint():
    """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í†µê³„"""
    try:
        stats = await memory_service.get_memory_statistics()
        MEMORY_OPERATIONS.labels(operation_type="stats_query").inc()
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@app.get("/mcp/tools/performance")
async def mcp_performance_endpoint():
    """MCP ë„êµ¬ ì„±ëŠ¥ í†µê³„"""
    try:
        stats = mcp_service.get_all_performance_stats()
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"MCP ì„±ëŠ¥ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

@app.get("/feedback/optimization-history")
async def optimization_history_endpoint():
    """ìµœì í™” ì´ë ¥ ì¡°íšŒ"""
    try:
        history = await feedback_service.get_optimization_history()# === Enhanced Agentic AI PoC Implementation ===
# PROJECT STRUCTURE:
# enhanced-agentic-ai-poc/
# â”œâ”€â”€ docker-compose.yml
# â”œâ”€â”€ .env.example
# â”œâ”€â”€ backend/
# â”‚   â”œâ”€â”€ Dockerfile
# â”‚   â”œâ”€â”€ requirements.txt
# â”‚   â”œâ”€â”€ main.py
# â”‚   â”œâ”€â”€ models/
# â”‚   â”‚   â””â”€â”€ schemas.py
# â”‚   â”œâ”€â”€ services/
# â”‚   â”‚   â”œâ”€â”€ memory_service.py
# â”‚   â”‚   â”œâ”€â”€ mcp_service.py
# â”‚   â”‚   â”œâ”€â”€ agent_service.py
# â”‚   â”‚   â””â”€â”€ feedback_service.py
# â”‚   â”œâ”€â”€ utils/
# â”‚   â”‚   â””â”€â”€ config.py
# â”‚   â””â”€â”€ mcp_tools/
# â”‚       â”œâ”€â”€ search_db.py
# â”‚       â”œâ”€â”€ send_slack.py
# â”‚       â”œâ”€â”€ generate_msg.py
# â”‚       â””â”€â”€ emergency_mail.py
# â”œâ”€â”€ frontend/
# â”‚   â”œâ”€â”€ Dockerfile
# â”‚   â”œâ”€â”€ requirements.txt
# â”‚   â””â”€â”€ streamlit_app.py
# â””â”€â”€ evaluation/
#     â”œâ”€â”€ poc_evaluator.py
#     â”œâ”€â”€ memory_tests.py
#     â”œâ”€â”€ feedback_tests.py
#     â””â”€â”€ performance_monitor.py

# === docker-compose.yml ===
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MEM0_API_KEY=${MEM0_API_KEY}
      - REDIS_URL=redis://redis:6379
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8001
      - NEO4J_URI=neo4j://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=testpassword
    depends_on:
      - redis
      - chroma
      - neo4j
    volumes:
      - ./backend:/app
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "8501:8501"
    environment:
      - BACKEND_URL=http://backend:8000
    depends_on:
      - backend
    volumes:
      - ./frontend:/app
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

  chroma:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma

  neo4j:
    image: neo4j:5
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/testpassword
      - NEO4J_PLUGINS=["apoc"]
    volumes:
      - neo4j_data:/data

  # ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

volumes:
  redis_data:
  chroma_data:
  neo4j_data:

# === .env.example ===
OPENAI_API_KEY=sk-your-openai-key-here
MEM0_API_KEY=your-mem0-key-here
ENVIRONMENT=development
LOG_LEVEL=INFO

# === backend/Dockerfile ===
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# === backend/requirements.txt ===
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
redis==5.0.1
chromadb==0.4.22
neo4j==5.15.0
openai==1.5.0
mem0ai==0.1.10
langgraph==0.2.5
langchain==0.2.18
langchain-openai==0.1.28
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
python-dotenv==1.0.0
asyncio==3.4.3
aioredis==2.0.1
prometheus-client==0.19.0
psutil==5.9.0

# === backend/utils/config.py ===
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    MEM0_API_KEY = os.getenv("MEM0_API_KEY")
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
    CHROMA_HOST = os.getenv("CHROMA_HOST", "localhost")
    CHROMA_PORT = os.getenv("CHROMA_PORT", "8001")
    NEO4J_URI = os.getenv("NEO4J_URI", "neo4j://localhost:7687")
    NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "testpassword")
    ENVIRONMENT = os.getenv("ENVIRONMENT", "development")
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

config = Config()

# === backend/models/schemas.py ===
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union
from enum import Enum
from datetime import datetime
import time

class AgentMode(str, Enum):
    FLOW = "flow"
    BASIC = "basic"

class MemoryType(str, Enum):
    WORKING = "working"
    EPISODIC = "episodic"
    SEMANTIC = "semantic"
    PROCEDURAL = "procedural"

class MCPToolType(str, Enum):
    SEARCH_DB = "search_db"
    SEND_SLACK = "send_slack"
    GENERATE_MSG = "generate_msg"
    EMERGENCY_MAIL = "emergency_mail_data_generator"
    SEND_EMAIL = "send_email"

# MCP Tool ê´€ë ¨ ìŠ¤í‚¤ë§ˆ
class MCPToolCall(BaseModel):
    tool_type: MCPToolType
    parameters: Dict[str, Any]
    context: Optional[Dict[str, Any]] = {}

class MCPToolResult(BaseModel):
    tool_type: MCPToolType
    success: bool
    result: Any
    execution_time: float
    error: Optional[str] = None

# ì›Œí¬í”Œë¡œìš° ê´€ë ¨ ìŠ¤í‚¤ë§ˆ
class WorkflowStep(BaseModel):
    step_id: int
    step_name: str
    action: str
    tool_calls: List[MCPToolCall]
    status: str = "pending"  # pending, running, completed, failed
    result: Optional[Any] = None
    execution_time: Optional[float] = None

class WorkflowPattern(BaseModel):
    pattern_id: str
    pattern_name: str
    steps: List[WorkflowStep]
    success_rate: float
    avg_execution_time: float
    usage_count: int
    last_used: datetime

# ë©”ëª¨ë¦¬ ê´€ë ¨ ìŠ¤í‚¤ë§ˆ
class ProceduralMemory(BaseModel):
    procedure_id: str
    name: str
    description: str
    workflow_pattern: WorkflowPattern
    success_rate: float
    optimization_history: List[Dict[str, Any]]
    created_at: datetime
    updated_at: datetime

class EpisodicMemory(BaseModel):
    episode_id: str
    user_id: str
    session_id: str
    timestamp: datetime
    interaction_type: str
    context: Dict[str, Any]
    workflow_executed: Optional[WorkflowPattern] = None
    user_satisfaction: Optional[float] = None
    lessons_learned: List[str] = []

class SemanticMemory(BaseModel):
    knowledge_id: str
    domain: str
    entity: str
    relation: str
    object: str
    confidence: float
    source: str
    created_at: datetime
    usage_count: int = 0

# í”¼ë“œë°± ê´€ë ¨ ìŠ¤í‚¤ë§ˆ
class PerformanceFeedback(BaseModel):
    feedback_id: str
    tool_type: MCPToolType
    execution_time: float
    success: bool
    error_type: Optional[str] = None
    optimization_applied: List[str] = []
    timestamp: datetime

class UserFeedback(BaseModel):
    feedback_id: str
    user_id: str
    session_id: str
    feedback_type: str  # satisfaction, preference, correction
    content: str
    rating: Optional[float] = None
    applied_changes: List[str] = []
    timestamp: datetime

# API ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class ChatRequest(BaseModel):
    message: str
    user_id: str
    session_id: str
    mode: AgentMode
    context: Optional[Dict[str, Any]] = {}
    preferred_tools: Optional[List[MCPToolType]] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    workflow_executed: Optional[WorkflowPattern] = None
    memory_used: Dict[MemoryType, List[str]]
    processing_time: float
    tools_used: List[MCPToolResult]
    confidence_score: float
    optimization_applied: List[str] = []

class FeedbackRequest(BaseModel):
    session_id: str
    user_id: str
    feedback_type: str
    content: str
    rating: Optional[float] = None
    target_component: Optional[str] = None  # workflow, tool, response

class FeedbackResponse(BaseModel):
    applied: bool
    processing_time: float
    optimizations: List[str]
    expected_improvements: Dict[str, float]

# í†µê³„ ë° ëª¨ë‹ˆí„°ë§ ìŠ¤í‚¤ë§ˆ
class SystemMetrics(BaseModel):
    timestamp: datetime
    memory_stats: Dict[MemoryType, Dict[str, Any]]
    tool_performance: Dict[MCPToolType, Dict[str, float]]
    workflow_efficiency: Dict[str, float]
    user_satisfaction: float
    system_load: Dict[str, float]

# === backend/mcp_tools/search_db.py ===
import asyncio
import time
import random
from typing import Dict, Any
from ..models.schemas import MCPToolResult, MCPToolType

class SearchDBTool:
    def __init__(self):
        self.tool_type = MCPToolType.SEARCH_DB
        self.performance_stats = {
            "avg_response_time": 1.2,
            "success_rate": 0.95,
            "query_limit": 100  # characters
        }
    
    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> MCPToolResult:
        """DB ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰"""
        start_time = time.time()
        
        query = parameters.get("query", "")
        table = parameters.get("table", "default")
        filters = parameters.get("filters", {})
        
        try:
            # ì¿¼ë¦¬ ê¸¸ì´ì— ë”°ë¥¸ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜
            if len(query) > self.performance_stats["query_limit"]:
                execution_time = 2.8 + random.uniform(0, 0.5)
                success_rate = 0.72
            else:
                execution_time = 1.2 + random.uniform(0, 0.3)
                success_rate = 0.95
            
            await asyncio.sleep(execution_time)
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
            success = random.random() < success_rate
            
            if success:
                # ëª¨ì˜ ê²°ê³¼ ë°ì´í„° ìƒì„±
                result_data = {
                    "query": query,
                    "table": table,
                    "results": [
                        {"id": i, "name": f"Item_{i}", "value": random.randint(1, 100)}
                        for i in range(random.randint(1, 5))
                    ],
                    "count": random.randint(1, 5)
                }
                
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=True,
                    result=result_data,
                    execution_time=time.time() - start_time
                )
            else:
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=False,
                    result=None,
                    execution_time=time.time() - start_time,
                    error="Database query failed - connection timeout"
                )
                
        except Exception as e:
            return MCPToolResult(
                tool_type=self.tool_type,
                success=False,
                result=None,
                execution_time=time.time() - start_time,
                error=str(e)
            )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        return self.performance_stats.copy()

# === backend/mcp_tools/send_slack.py ===
import asyncio
import time
import random
from typing import Dict, Any
from ..models.schemas import MCPToolResult, MCPToolType

class SendSlackTool:
    def __init__(self):
        self.tool_type = MCPToolType.SEND_SLACK
        self.performance_stats = {
            "avg_response_time": 0.8,
            "success_rate": 0.92,
            "message_limit": 4000,  # characters
            "api_rate_limit": 50  # per minute
        }
        self.api_calls_count = 0
        self.last_reset_time = time.time()
    
    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> MCPToolResult:
        """Slack ë©”ì‹œì§€ ì „ì†¡ ë„êµ¬ ì‹¤í–‰"""
        start_time = time.time()
        
        message = parameters.get("message", "")
        channel = parameters.get("channel", "#general")
        user = parameters.get("user", None)
        
        try:
            # API í˜¸ì¶œ ì œí•œ í™•ì¸
            current_time = time.time()
            if current_time - self.last_reset_time > 60:  # 1ë¶„ ê²½ê³¼
                self.api_calls_count = 0
                self.last_reset_time = current_time
            
            if self.api_calls_count >= self.performance_stats["api_rate_limit"]:
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=False,
                    result=None,
                    execution_time=time.time() - start_time,
                    error="API rate limit exceeded"
                )
            
            # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ í™•ì¸
            if len(message) > self.performance_stats["message_limit"]:
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=False,
                    result=None,
                    execution_time=time.time() - start_time,
                    error=f"Message too long: {len(message)} > {self.performance_stats['message_limit']}"
                )
            
            # ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            execution_time = 0.8 + random.uniform(0, 0.4)
            await asyncio.sleep(execution_time)
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
            success = random.random() < self.performance_stats["success_rate"]
            self.api_calls_count += 1
            
            if success:
                result_data = {
                    "message": message,
                    "channel": channel,
                    "timestamp": current_time,
                    "message_id": f"slack_msg_{int(current_time * 1000)}"
                }
                
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=True,
                    result=result_data,
                    execution_time=time.time() - start_time
                )
            else:
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=False,
                    result=None,
                    execution_time=time.time() - start_time,
                    error="Slack API error - channel not found"
                )
                
        except Exception as e:
            return MCPToolResult(
                tool_type=self.tool_type,
                success=False,
                result=None,
                execution_time=time.time() - start_time,
                error=str(e)
            )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        return {
            **self.performance_stats,
            "current_api_calls": self.api_calls_count,
            "time_until_reset": max(0, 60 - (time.time() - self.last_reset_time))
        }

# === backend/mcp_tools/generate_msg.py ===
import asyncio
import time
import random
from typing import Dict, Any
from openai import OpenAI
from ..models.schemas import MCPToolResult, MCPToolType
from ..utils.config import config

class GenerateMessageTool:
    def __init__(self):
        self.tool_type = MCPToolType.GENERATE_MSG
        self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.performance_stats = {
            "avg_response_time": 2.1,
            "success_rate": 0.94,
            "template_cache": {}
        }
    
    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> MCPToolResult:
        """ë©”ì‹œì§€ ìƒì„± ë„êµ¬ ì‹¤í–‰"""
        start_time = time.time()
        
        message_type = parameters.get("type", "general")
        content = parameters.get("content", "")
        style = parameters.get("style", "professional")  # professional, casual, technical
        length = parameters.get("length", "medium")  # short, medium, long
        
        try:
            # í…œí”Œë¦¿ ìºì‹± ìµœì í™” ì‹œë®¬ë ˆì´ì…˜
            cache_key = f"{message_type}_{style}_{length}"
            if cache_key in self.performance_stats["template_cache"]:
                execution_time = 1.1 + random.uniform(0, 0.2)  # ìºì‹œ ì ìš©ìœ¼ë¡œ ë¹ ë¦„
            else:
                execution_time = 2.1 + random.uniform(0, 0.5)  # ìƒˆë¡œ ìƒì„±
                self.performance_stats["template_cache"][cache_key] = True
            
            await asyncio.sleep(execution_time)
            
            # ìŠ¤íƒ€ì¼ë³„ ë©”ì‹œì§€ ìƒì„± ë¡œì§
            if style == "professional":
                generated_message = f"ì•ˆë…•í•˜ì„¸ìš”,\n\n{content}ì— ëŒ€í•´ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.\n\nê°ì‚¬í•©ë‹ˆë‹¤."
            elif style == "casual":
                generated_message = f"ì•ˆë…•! {content} ê´€ë ¨í•´ì„œ ì•Œë ¤ì¤„ê²Œ ğŸ˜Š"
            elif style == "technical":
                generated_message = f"[ì‹œìŠ¤í…œ ì•Œë¦¼] {content}\n\nì„¸ë¶€ ì •ë³´:\n- íƒ€ì„ìŠ¤íƒ¬í”„: {time.strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                generated_message = content
            
            # ê¸¸ì´ ì¡°ì •
            if length == "short":
                generated_message = generated_message[:100] + "..." if len(generated_message) > 100 else generated_message
            elif length == "long":
                generated_message += "\n\nì¶”ê°€ì ìœ¼ë¡œ í•„ìš”í•œ ì •ë³´ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”."
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
            success = random.random() < self.performance_stats["success_rate"]
            
            if success:
                result_data = {
                    "generated_message": generated_message,
                    "style": style,
                    "length": length,
                    "word_count": len(generated_message.split()),
                    "cached": cache_key in self.performance_stats["template_cache"]
                }
                
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=True,
                    result=result_data,
                    execution_time=time.time() - start_time
                )
            else:
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=False,
                    result=None,
                    execution_time=time.time() - start_time,
                    error="Message generation failed - content policy violation"
                )
                
        except Exception as e:
            return MCPToolResult(
                tool_type=self.tool_type,
                success=False,
                result=None,
                execution_time=time.time() - start_time,
                error=str(e)
            )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        return {
            **self.performance_stats,
            "cached_templates": len(self.performance_stats["template_cache"])
        }

# === backend/mcp_tools/emergency_mail.py ===
import asyncio
import time
import random
from typing import Dict, Any
from ..models.schemas import MCPToolResult, MCPToolType

class EmergencyMailDataGeneratorTool:
    def __init__(self):
        self.tool_type = MCPToolType.EMERGENCY_MAIL
        self.performance_stats = {
            "avg_response_time": 0.9,
            "success_rate": 0.98,
            "data_templates": {
                "she_emergency": {
                    "priority": "high",
                    "recipients": ["safety@company.com", "manager@company.com"],
                    "template": "SHE ë¹„ìƒ ìƒí™© ë°œìƒ"
                },
                "system_emergency": {
                    "priority": "critical",
                    "recipients": ["ops@company.com", "admin@company.com"],
                    "template": "ì‹œìŠ¤í…œ ë¹„ìƒ ìƒí™© ë°œìƒ"
                }
            }
        }
    
    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> MCPToolResult:
        """ë¹„ìƒ ë©”ì¼ ë°ì´í„° ìƒì„± ë„êµ¬ ì‹¤í–‰"""
        start_time = time.time()
        
        emergency_type = parameters.get("type", "general")
        severity = parameters.get("severity", "medium")
        location = parameters.get("location", "unknown")
        description = parameters.get("description", "")
        
        try:
            # ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            execution_time = 0.9 + random.uniform(0, 0.2)
            await asyncio.sleep(execution_time)
            
            # ë¹„ìƒ ìœ í˜•ë³„ ë°ì´í„° ìƒì„±
            template_data = self.performance_stats["data_templates"].get(
                emergency_type, 
                self.performance_stats["data_templates"]["system_emergency"]
            )
            
            # ì‹¬ê°ë„ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ì¡°ì •
            priority_map = {"low": "medium", "medium": "high", "high": "critical"}
            final_priority = priority_map.get(severity, "high")
            
            # ìˆ˜ì‹ ì ëª©ë¡ ìƒì„±
            recipients = template_data["recipients"].copy()
            if final_priority == "critical":
                recipients.extend(["ceo@company.com", "emergency@company.com"])
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
            success = random.random() < self.performance_stats["success_rate"]
            
            if success:
                result_data = {
                    "emergency_id": f"EMG_{int(time.time() * 1000)}",
                    "type": emergency_type,
                    "priority": final_priority,
                    "severity": severity,
                    "location": location,
                    "description": description,
                    "recipients": recipients,
                    "template": template_data["template"],
                    "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "escalation_required": final_priority == "critical"
                }
                
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=True,
                    result=result_data,
                    execution_time=time.time() - start_time
                )
            else:
                return MCPToolResult(
                    tool_type=self.tool_type,
                    success=False,
                    result=None,
                    execution_time=time.time() - start_time,
                    error="Emergency data generation failed - template not found"
                )
                
        except Exception as e:
            return MCPToolResult(
                tool_type=self.tool_type,
                success=False,
                result=None,
                execution_time=time.time() - start_time,
                error=str(e)
            )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        return self.performance_stats.copy()

# === backend/services/mcp_service.py ===
import asyncio
from typing import Dict, Any, List, Type
from ..models.schemas import MCPToolCall, MCPToolResult, MCPToolType
from ..mcp_tools.search_db import SearchDBTool
from ..mcp_tools.send_slack import SendSlackTool
from ..mcp_tools.generate_msg import GenerateMessageTool
from ..mcp_tools.emergency_mail import EmergencyMailDataGeneratorTool

class MCPService:
    def __init__(self):
        self.tools = {
            MCPToolType.SEARCH_DB: SearchDBTool(),
            MCPToolType.SEND_SLACK: SendSlackTool(),
            MCPToolType.GENERATE_MSG: GenerateMessageTool(),
            MCPToolType.EMERGENCY_MAIL: EmergencyMailDataGeneratorTool(),
        }
        self.performance_history = []
    
    async def execute_tool(self, tool_call: MCPToolCall) -> MCPToolResult:
        """MCP ë„êµ¬ ì‹¤í–‰"""
        if tool_call.tool_type not in self.tools:
            return MCPToolResult(
                tool_type=tool_call.tool_type,
                success=False,
                result=None,
                execution_time=0,
                error=f"Tool {tool_call.tool_type} not found"
            )
        
        tool = self.tools[tool_call.tool_type]
        result = await tool.execute(tool_call.parameters, tool_call.context)
        
        # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self.performance_history.append({
            "tool_type": tool_call.tool_type,
            "execution_time": result.execution_time,
            "success": result.success,
            "timestamp": asyncio.get_event_loop().time()
        })
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.performance_history) > 1000:
            self.performance_history = self.performance_history[-500:]
        
        return result
    
    async def execute_workflow(self, tool_calls: List[MCPToolCall]) -> List[MCPToolResult]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ë„êµ¬ ì²´ì¸)"""
        results = []
        
        for tool_call in tool_calls:
            # ì´ì „ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë„êµ¬ì˜ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
            if results:
                tool_call.context = tool_call.context or {}
                tool_call.context["previous_results"] = [r.result for r in results if r.success]
            
            result = await self.execute_tool(tool_call)
            results.append(result)
            
            # ì‹¤íŒ¨ ì‹œ ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨ (ì˜µì…˜)
            if not result.success:
                break
        
        return results
    
    def get_tool_performance_stats(self, tool_type: MCPToolType) -> Dict[str, Any]:
        """íŠ¹ì • ë„êµ¬ì˜ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if tool_type not in self.tools:
            return {}
        
        tool_data = [h for h in self.performance_history if h["tool_type"] == tool_type]
        
        if not tool_data:
            return self.tools[tool_type].get_performance_stats()
        
        success_count = sum(1 for d in tool_data if d["success"])
        avg_time = sum(d["execution_time"] for d in tool_data) / len(tool_data)
        
        return {
            **self.tools[tool_type].get_performance_stats(),
            "recent_success_rate": success_count / len(tool_data),
            "recent_avg_time": avg_time,
            "usage_count": len(tool_data)
        }
    
    def get_all_performance_stats(self) -> Dict[MCPToolType, Dict[str, Any]]:
        """ëª¨ë“  ë„êµ¬ì˜ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return {
            tool_type: self.get_tool_performance_stats(tool_type)
            for tool_type in self.tools.keys()
        }
    
    def suggest_optimal_tool_combination(self, task_description: str) -> List[MCPToolType]:
        """ì‘ì—… ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì  ë„êµ¬ ì¡°í•© ì œì•ˆ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP í•„ìš”)
        keywords = task_description.lower()
        
        suggested_tools = []
        
        if any(word in keywords for word in ["ë©”ì‹œì§€", "ìƒì„±", "ì‘ì„±"]):
            suggested_tools.append(MCPToolType.GENERATE_MSG)
        
        if any(word in keywords for word in ["slack", "ì•Œë¦¼", "ì „ì†¡", "ë°œì†¡"]):
            suggested_tools.append(MCPToolType.SEND_SLACK)
        
        return suggested_tools

# === backend/services/memory_service.py ===
import asyncio
import json
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import redis.asyncio as redis
from mem0 import Memory
from chromadb import Client
from neo4j import GraphDatabase
from ..utils.config import config
from ..models.schemas import (
    MemoryType, ProceduralMemory, EpisodicMemory, SemanticMemory,
    WorkflowPattern, MCPToolType
)

class EnhancedMemoryService:
    def __init__(self):
        self.redis_client = None
        self.mem0 = Memory()
        self.chroma_client = None
        self.neo4j_driver = None
        self.performance_metrics = []
        
    async def initialize(self):
        """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # Redis - Working Memory
        self.redis_client = redis.from_url(config.REDIS_URL)
        
        # ChromaDB - Episodic Memory
        self.chroma_client = Client(host=config.CHROMA_HOST, port=int(config.CHROMA_PORT))
        try:
            self.episodes_collection = self.chroma_client.create_collection("episodes")
            self.procedures_collection = self.chroma_client.create_collection("procedures")
        except:
            self.episodes_collection = self.chroma_client.get_collection("episodes")
            self.procedures_collection = self.chroma_client.get_collection("procedures")
        
        # Neo4j - Semantic Memory
        self.neo4j_driver = GraphDatabase.driver(
            config.NEO4J_URI,
            auth=(config.NEO4J_USER, config.NEO4J_PASSWORD)
        )
    
    # === Working Memory (ì‘ì—… ê¸°ì–µ) ===
    async def store_working_memory(self, session_id: str, key: str, value: Any, ttl: int = 3600):
        """ì„¸ì…˜ë³„ ì‘ì—… ë©”ëª¨ë¦¬ ì €ì¥"""
        start_time = time.time()
        
        memory_key = f"working:{session_id}:{key}"
        await self.redis_client.setex(memory_key, ttl, json.dumps(value, default=str))
        
        return time.time() - start_time
    
    async def get_working_memory(self, session_id: str, key: str = None) -> Dict[str, Any]:
        """ì„¸ì…˜ë³„ ì‘ì—… ë©”ëª¨ë¦¬ ì¡°íšŒ"""
        start_time = time.time()
        
        if key:
            memory_key = f"working:{session_id}:{key}"
            result = await self.redis_client.get(memory_key)
            if result:
                return {key: json.loads(result)}, time.time() - start_time
            return {}, time.time() - start_time
        else:
            # ì„¸ì…˜ì˜ ëª¨ë“  ì‘ì—… ë©”ëª¨ë¦¬ ì¡°íšŒ
            pattern = f"working:{session_id}:*"
            keys = await self.redis_client.keys(pattern)
            
            result = {}
            for full_key in keys:
                key_name = full_key.decode().split(":")[-1]
                value = await self.redis_client.get(full_key)
                if value:
                    result[key_name] = json.loads(value)
            
            return result, time.time() - start_time
    
    # === Procedural Memory (ì ˆì°¨ì  ê¸°ì–µ) ===
    async def store_procedural_memory(self, workflow_pattern: WorkflowPattern, user_id: str):
        """ì„±ê³µí•œ ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ì €ì¥"""
        start_time = time.time()
        
        # Mem0ì— ì ˆì°¨ íŒ¨í„´ ì €ì¥
        procedure_data = {
            "pattern_id": workflow_pattern.pattern_id,
            "pattern_name": workflow_pattern.pattern_name,
            "steps": [step.dict() for step in workflow_pattern.steps],
            "success_rate": workflow_pattern.success_rate,
            "avg_execution_time": workflow_pattern.avg_execution_time,
            "usage_count": workflow_pattern.usage_count
        }
        
        self.mem0.add(
            f"Workflow pattern: {workflow_pattern.pattern_name} with {len(workflow_pattern.steps)} steps, "
            f"success rate: {workflow_pattern.success_rate:.1%}",
            user_id=user_id,
            metadata={
                "type": "procedural",
                "pattern_id": workflow_pattern.pattern_id,
                "domain": "workflow"
            }
        )
        
        # ChromaDBì—ë„ ìƒì„¸ ì •ë³´ ì €ì¥
        self.procedures_collection.add(
            documents=[json.dumps(procedure_data, default=str)],
            metadatas=[{
                "pattern_id": workflow_pattern.pattern_id,
                "user_id": user_id,
                "success_rate": workflow_pattern.success_rate,
                "created_at": datetime.now().isoformat()
            }],
            ids=[f"{user_id}_{workflow_pattern.pattern_id}"]
        )
        
        return time.time() - start_time
    
    async def retrieve_similar_procedures(self, task_description: str, user_id: str, limit: int = 3):
        """ìœ ì‚¬í•œ ì ˆì°¨ íŒ¨í„´ ê²€ìƒ‰"""
        start_time = time.time()
        
        # Mem0ì—ì„œ ê´€ë ¨ ì ˆì°¨ ê²€ìƒ‰
        memories = self.mem0.search(
            query=f"workflow procedure for: {task_description}",
            user_id=user_id,
            limit=limit
        )
        
        procedures = []
        if memories and "results" in memories:
            for memory in memories["results"]:
                metadata = memory.get("metadata", {})
                if metadata.get("type") == "procedural":
                    procedures.append({
                        "content": memory.get("memory", ""),
                        "pattern_id": metadata.get("pattern_id"),
                        "relevance_score": memory.get("score", 0)
                    })
        
        return procedures, time.time() - start_time
    
    # === Episodic Memory (ì¼í™”ì  ê¸°ì–µ) ===
    async def store_episodic_memory(self, episode: EpisodicMemory):
        """ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì—í”¼ì†Œë“œ ì €ì¥"""
        start_time = time.time()
        
        # Mem0ì— ì—í”¼ì†Œë“œ ì •ë³´ ì €ì¥
        episode_content = (
            f"User {episode.user_id} interaction: {episode.interaction_type}. "
            f"Context: {episode.context}. "
            f"Satisfaction: {episode.user_satisfaction}"
        )
        
        self.mem0.add(
            episode_content,
            user_id=episode.user_id,
            metadata={
                "type": "episodic",
                "episode_id": episode.episode_id,
                "interaction_type": episode.interaction_type,
                "timestamp": episode.timestamp.isoformat()
            }
        )
        
        # ChromaDBì— ìƒì„¸ ì—í”¼ì†Œë“œ ì €ì¥
        episode_data = episode.dict()
        self.episodes_collection.add(
            documents=[json.dumps(episode_data, default=str)],
            metadatas=[{
                "user_id": episode.user_id,
                "episode_id": episode.episode_id,
                "interaction_type": episode.interaction_type,
                "timestamp": episode.timestamp.isoformat(),
                "satisfaction": episode.user_satisfaction or 0
            }],
            ids=[episode.episode_id]
        )
        
        return time.time() - start_time
    
    async def retrieve_user_episodes(self, user_id: str, interaction_type: str = None, limit: int = 5):
        """ì‚¬ìš©ìì˜ ê³¼ê±° ì—í”¼ì†Œë“œ ê²€ìƒ‰"""
        start_time = time.time()
        
        query = f"user {user_id} interactions"
        if interaction_type:
            query += f" related to {interaction_type}"
        
        memories = self.mem0.search(
            query=query,
            user_id=user_id,
            limit=limit
        )
        
        episodes = []
        if memories and "results" in memories:
            for memory in memories["results"]:
                metadata = memory.get("metadata", {})
                if metadata.get("type") == "episodic":
                    episodes.append({
                        "content": memory.get("memory", ""),
                        "episode_id": metadata.get("episode_id"),
                        "interaction_type": metadata.get("interaction_type"),
                        "timestamp": metadata.get("timestamp"),
                        "relevance_score": memory.get("score", 0)
                    })
        
        return episodes, time.time() - start_time
    
    # === Semantic Memory (ì˜ë¯¸ì  ê¸°ì–µ) ===
    async def store_semantic_knowledge(self, knowledge: SemanticMemory):
        """ë„ë©”ì¸ ì§€ì‹ ì €ì¥"""
        start_time = time.time()
        
        # Neo4jì— ì§€ì‹ ê·¸ë˜í”„ë¡œ ì €ì¥
        with self.neo4j_driver.session() as session:
            query = """
            MERGE (e:Entity {name: $entity, domain: $domain})
            MERGE (o:Entity {name: $object, domain: $domain})
            MERGE (e)-[r:RELATION {
                type: $relation,
                confidence: $confidence,
                source: $source,
                created_at: $created_at,
                knowledge_id: $knowledge_id
            }]->(o)
            SET r.usage_count = COALESCE(r.usage_count, 0) + 1
            """
            
            session.run(query, {
                "entity": knowledge.entity,
                "object": knowledge.object,
                "relation": knowledge.relation,
                "domain": knowledge.domain,
                "confidence": knowledge.confidence,
                "source": knowledge.source,
                "created_at": knowledge.created_at.isoformat(),
                "knowledge_id": knowledge.knowledge_id
            })
        
        # Mem0ì—ë„ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
        knowledge_text = f"{knowledge.entity} {knowledge.relation} {knowledge.object} in {knowledge.domain} domain"
        self.mem0.add(
            knowledge_text,
            user_id="system",  # ì‹œìŠ¤í…œ ë ˆë²¨ ì§€ì‹
            metadata={
                "type": "semantic",
                "domain": knowledge.domain,
                "knowledge_id": knowledge.knowledge_id,
                "confidence": knowledge.confidence
            }
        )
        
        return time.time() - start_time
    
    async def query_semantic_knowledge(self, entity: str, domain: str = None, limit: int = 10):
        """ì˜ë¯¸ì  ì§€ì‹ ì¿¼ë¦¬"""
        start_time = time.time()
        
        with self.neo4j_driver.session() as session:
            if domain:
                query = """
                MATCH (e:Entity {name: $entity, domain: $domain})-[r:RELATION]->(o:Entity)
                RETURN e.name as entity, r.type as relation, o.name as object, 
                       r.confidence as confidence, r.usage_count as usage_count
                ORDER BY r.confidence DESC, r.usage_count DESC
                LIMIT $limit
                """
                result = session.run(query, {"entity": entity, "domain": domain, "limit": limit})
            else:
                query = """
                MATCH (e:Entity {name: $entity})-[r:RELATION]->(o:Entity)
                RETURN e.name as entity, r.type as relation, o.name as object, 
                       r.confidence as confidence, r.usage_count as usage_count, e.domain as domain
                ORDER BY r.confidence DESC, r.usage_count DESC
                LIMIT $limit
                """
                result = session.run(query, {"entity": entity, "limit": limit})
            
            knowledge_items = []
            for record in result:
                knowledge_items.append({
                    "entity": record["entity"],
                    "relation": record["relation"],
                    "object": record["object"],
                    "confidence": record["confidence"],
                    "usage_count": record["usage_count"],
                    "domain": record.get("domain", domain)
                })
        
        return knowledge_items, time.time() - start_time
    
    # === í†µí•© ê²€ìƒ‰ ë©”ì†Œë“œ ===
    async def retrieve_relevant_memories(self, user_id: str, query: str, memory_types: List[MemoryType] = None, limit: int = 5):
        """ê´€ë ¨ ë©”ëª¨ë¦¬ í†µí•© ê²€ìƒ‰"""
        start_time = time.time()
        all_memories = {
            MemoryType.WORKING: [],
            MemoryType.EPISODIC: [],
            MemoryType.SEMANTIC: [],
            MemoryType.PROCEDURAL: []
        }
        
        # ìš”ì²­ëœ ë©”ëª¨ë¦¬ íƒ€ì…ë§Œ ê²€ìƒ‰ (ê¸°ë³¸ì€ ëª¨ë“  íƒ€ì…)
        if memory_types is None:
            memory_types = list(MemoryType)
        
        # Mem0 í†µí•© ê²€ìƒ‰
        memories = self.mem0.search(query=query, user_id=user_id, limit=limit * len(memory_types))
        
        if memories and "results" in memories:
            for memory in memories["results"]:
                metadata = memory.get("metadata", {})
                memory_type = metadata.get("type", "")
                
                memory_item = {
                    "content": memory.get("memory", ""),
                    "score": memory.get("score", 0),
                    "metadata": metadata
                }
                
                if memory_type == "working" and MemoryType.WORKING in memory_types:
                    all_memories[MemoryType.WORKING].append(memory_item)
                elif memory_type == "episodic" and MemoryType.EPISODIC in memory_types:
                    all_memories[MemoryType.EPISODIC].append(memory_item)
                elif memory_type == "semantic" and MemoryType.SEMANTIC in memory_types:
                    all_memories[MemoryType.SEMANTIC].append(memory_item)
                elif memory_type == "procedural" and MemoryType.PROCEDURAL in memory_types:
                    all_memories[MemoryType.PROCEDURAL].append(memory_item)
        
        # ê° íƒ€ì…ë³„ë¡œ ìµœê³  ì ìˆ˜ í•­ëª©ë“¤ë§Œ ìœ ì§€
        for memory_type in all_memories:
            all_memories[memory_type] = sorted(
                all_memories[memory_type], 
                key=lambda x: x["score"], 
                reverse=True
            )[:limit]
        
        return all_memories, time.time() - start_time
    
    async def get_memory_statistics(self):
        """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ í†µê³„"""
        stats = {}
        
        try:
            # Redis í†µê³„
            redis_info = await self.redis_client.info()
            working_keys = await self.redis_client.keys("working:*")
            stats["working_memory"] = {
                "total_keys": len(working_keys),
                "memory_usage": redis_info.get("used_memory_human", "0"),
                "connected_clients": redis_info.get("connected_clients", 0)
            }
            
            # ChromaDB í†µê³„
            stats["episodic_memory"] = {
                "episodes_count": self.episodes_collection.count(),
                "procedures_count": self.procedures_collection.count()
            }
            
            # Neo4j í†µê³„
            with self.neo4j_driver.session() as session:
                result = session.run("MATCH (n) RETURN count(n) as node_count")
                node_count = result.single()["node_count"]
                
                result = session.run("MATCH ()-[r]->() RETURN count(r) as rel_count")
                rel_count = result.single()["rel_count"]
                
                stats["semantic_memory"] = {
                    "nodes_count": node_count,
                    "relationships_count": rel_count
                }
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            if self.performance_metrics:
                recent_metrics = self.performance_metrics[-10:]
                avg_time = sum(m.memory_retrieval_time for m in recent_metrics) / len(recent_metrics)
                stats["performance"] = {
                    "avg_retrieval_time": avg_time,
                    "total_operations": len(self.performance_metrics)
                }
            
        except Exception as e:
            stats["error"] = str(e)
        
        return stats

# === backend/services/feedback_service.py ===
import asyncio
import time
import json
from typing import Dict, Any, List
from collections import defaultdict
from datetime import datetime, timedelta
from ..services.memory_service import EnhancedMemoryService
from ..models.schemas import (
    FeedbackRequest, FeedbackResponse, PerformanceFeedback, UserFeedback,
    MCPToolType, WorkflowPattern
)

class EnhancedFeedbackService:
    def __init__(self, memory_service: EnhancedMemoryService):
        self.memory_service = memory_service
        self.user_preferences = defaultdict(dict)
        self.performance_feedback = []
        self.optimization_history = defaultdict(list)
        
    async def process_immediate_feedback(self, feedback: FeedbackRequest) -> FeedbackResponse:
        """5ì´ˆ ì´ë‚´ ì¦‰ì‹œ í”¼ë“œë°± ì²˜ë¦¬"""
        start_time = time.time()
        
        optimizations = []
        
        try:
            # ì‚¬ìš©ì í”¼ë“œë°± ì €ì¥
            user_feedback = UserFeedback(
                feedback_id=f"fb_{int(time.time() * 1000)}",
                user_id=feedback.user_id,
                session_id=feedback.session_id,
                feedback_type=feedback.feedback_type,
                content=feedback.content,
                rating=feedback.rating,
                timestamp=datetime.now()
            )
            
            # í”¼ë“œë°± ìœ í˜•ë³„ ì¦‰ì‹œ ì ìš©
            if feedback.feedback_type == "style_preference":
                # ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ ì„ í˜¸ë„ ì—…ë°ì´íŠ¸
                style = self._extract_style_preference(feedback.content)
                self.user_preferences[feedback.user_id]["message_style"] = style
                optimizations.append(f"ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ì„ {style}ë¡œ ë³€ê²½")
                
                # Working Memoryì— ì¦‰ì‹œ ì ìš©
                await self.memory_service.store_working_memory(
                    session_id=feedback.session_id,
                    key="style_preference",
                    value=style,
                    ttl=1800
                )
                
            elif feedback.feedback_type == "tool_performance":
                # ë„êµ¬ ì„±ëŠ¥ í”¼ë“œë°± ì²˜ë¦¬
                tool_optimization = self._process_tool_feedback(feedback.content)
                if tool_optimization:
                    optimizations.extend(tool_optimization)
                    
            elif feedback.feedback_type == "workflow_efficiency":
                # ì›Œí¬í”Œë¡œìš° íš¨ìœ¨ì„± í”¼ë“œë°±
                workflow_improvements = await self._optimize_workflow_from_feedback(
                    feedback.user_id, 
                    feedback.session_id, 
                    feedback.content
                )
                optimizations.extend(workflow_improvements)
                
            elif feedback.feedback_type == "response_quality":
                # ì‘ë‹µ í’ˆì§ˆ ê°œì„ 
                quality_improvements = self._improve_response_quality(
                    feedback.content, 
                    feedback.rating or 3.0
                )
                optimizations.extend(quality_improvements)
            
            # í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ í•™ìŠµì„ ìœ„í•œ ì„ í˜¸ë„ ê³µìœ 
            await self._share_user_preferences(feedback.user_id)
            
            # ì¥ê¸° ë©”ëª¨ë¦¬ì— í”¼ë“œë°± ì €ì¥
            await self._store_feedback_in_memory(user_feedback)
            
            processing_time = time.time() - start_time
            
            # ì˜ˆìƒ ê°œì„  íš¨ê³¼ ê³„ì‚°
            expected_improvements = self._calculate_expected_improvements(optimizations)
            
            return FeedbackResponse(
                applied=True,
                processing_time=processing_time,
                optimizations=optimizations,
                expected_improvements=expected_improvements
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            return FeedbackResponse(
                applied=False,
                processing_time=processing_time,
                optimizations=[f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                expected_improvements={}
            )
    
    async def process_performance_feedback(self, tool_type: MCPToolType, execution_time: float, success: bool, error_type: str = None):
        """ë„êµ¬ ì„±ëŠ¥ í”¼ë“œë°± ì²˜ë¦¬"""
        feedback = PerformanceFeedback(
            feedback_id=f"perf_{int(time.time() * 1000)}",
            tool_type=tool_type,
            execution_time=execution_time,
            success=success,
            error_type=error_type,
            timestamp=datetime.now()
        )
        
        self.performance_feedback.append(feedback)
        
        # ì„±ëŠ¥ ìµœì í™” ë¡œì§
        optimizations = []
        
        # ì‹¤í–‰ ì‹œê°„ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
        if execution_time > 3.0:  # 3ì´ˆ ì´ìƒ
            optimization = await self._optimize_slow_tool(tool_type, execution_time)
            if optimization:
                optimizations.extend(optimization)
                feedback.optimization_applied = optimization
        
        # ì‹¤íŒ¨ìœ¨ì´ ë†’ì€ ê²½ìš°
        recent_failures = [f for f in self.performance_feedback[-20:] 
                          if f.tool_type == tool_type and not f.success]
        
        if len(recent_failures) >= 5:  # ìµœê·¼ 20íšŒ ì¤‘ 5íšŒ ì´ìƒ ì‹¤íŒ¨
            reliability_optimization = await self._improve_tool_reliability(tool_type)
            optimizations.extend(reliability_optimization)
            feedback.optimization_applied.extend(reliability_optimization)
        
        return optimizations
    
    def _extract_style_preference(self, feedback_content: str) -> str:
        """í”¼ë“œë°±ì—ì„œ ìŠ¤íƒ€ì¼ ì„ í˜¸ë„ ì¶”ì¶œ"""
        content_lower = feedback_content.lower()
        
        if any(word in content_lower for word in ["ì¹œê·¼", "ìºì£¼ì–¼", "í¸í•œ"]):
            return "casual"
        elif any(word in content_lower for word in ["ê³µì‹", "ì •ì¤‘", "ë¹„ì¦ˆë‹ˆìŠ¤"]):
            return "professional"
        elif any(word in content_lower for word in ["ê¸°ìˆ ì ", "ìì„¸í•œ", "ì „ë¬¸ì "]):
            return "technical"
        elif any(word in content_lower for word in ["ê°„ë‹¨", "ì§§ê²Œ", "ìš”ì•½"]):
            return "concise"
        else:
            return "balanced"
    
    def _process_tool_feedback(self, feedback_content: str) -> List[str]:
        """ë„êµ¬ ì„±ëŠ¥ í”¼ë“œë°± ì²˜ë¦¬"""
        optimizations = []
        content_lower = feedback_content.lower()
        
        if any(word in content_lower for word in ["ëŠë ¤", "ì˜¤ë˜", "ì§€ì—°"]):
            optimizations.append("ë„êµ¬ ì‹¤í–‰ ì†ë„ ìµœì í™” ì ìš©")
            optimizations.append("ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”")
            
        elif any(word in content_lower for word in ["ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì—ëŸ¬"]):
            optimizations.append("ë„êµ¬ ì•ˆì •ì„± ê°œì„ ")
            optimizations.append("ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”")
            
        elif any(word in content_lower for word in ["ë¶€ì •í™•", "í‹€ë¦°", "ì˜ëª»"]):
            optimizations.append("ë„êµ¬ ì •í™•ë„ ê°œì„ ")
            optimizations.append("ê²€ì¦ ë¡œì§ ì¶”ê°€")
        
        return optimizations
    
    async def _optimize_workflow_from_feedback(self, user_id: str, session_id: str, feedback_content: str) -> List[str]:
        """ì›Œí¬í”Œë¡œìš° í”¼ë“œë°± ê¸°ë°˜ ìµœì í™”"""
        optimizations = []
        
        # í˜„ì¬ ì„¸ì…˜ì˜ ì›Œí¬í”Œë¡œìš° ì •ë³´ ì¡°íšŒ
        workflow_data, _ = await self.memory_service.get_working_memory(session_id, "current_workflow")
        
        if workflow_data:
            content_lower = feedback_content.lower()
            
            if any(word in content_lower for word in ["ë‹¨ê³„", "ë³µì¡", "ê°„ì†Œí™”"]):
                optimizations.append("ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ìµœì í™”")
                # ë¶ˆí•„ìš”í•œ ë‹¨ê³„ ì œê±° ë¡œì§
                
            elif any(word in content_lower for word in ["ìˆœì„œ", "íë¦„", "ë¡œì§"]):
                optimizations.append("ì‹¤í–‰ ìˆœì„œ ì¬ë°°ì¹˜")
                # ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë‹¨ê³„ ì‹ë³„
                
            elif any(word in content_lower for word in ["ë„êµ¬", "ì„ íƒ", "ëŒ€ì²´"]):
                optimizations.append("ìµœì  ë„êµ¬ ì¡°í•© ì¬ì„ íƒ")
                # ì„±ëŠ¥ì´ ë” ì¢‹ì€ ë„êµ¬ë¡œ ëŒ€ì²´
        
        return optimizations
    
    def _improve_response_quality(self, feedback_content: str, rating: float) -> List[str]:
        """ì‘ë‹µ í’ˆì§ˆ ê°œì„ """
        optimizations = []
        
        if rating < 3.0:
            content_lower = feedback_content.lower()
            
            if any(word in content_lower for word in ["ê¸¸ì–´", "ì¥í™©", "ë„ˆë¬´"]):
                optimizations.append("ì‘ë‹µ ê¸¸ì´ ìµœì í™”")
                
            elif any(word in content_lower for word in ["ì§§ì•„", "ë¶€ì¡±", "ë”"]):
                optimizations.append("ì‘ë‹µ ìƒì„¸ë„ ì¦ê°€")
                
            elif any(word in content_lower for word in ["ê´€ë ¨ì—†", "ë¶€ì •í™•", "í‹€ë¦°"]):
                optimizations.append("ì‘ë‹µ ê´€ë ¨ì„± ê°œì„ ")
                optimizations.append("ì»¨í…ìŠ¤íŠ¸ ì´í•´ë„ í–¥ìƒ")
        
        return optimizations
    
    async def _optimize_slow_tool(self, tool_type: MCPToolType, execution_time: float) -> List[str]:
        """ëŠë¦° ë„êµ¬ ìµœì í™”"""
        optimizations = []
        
        if tool_type == MCPToolType.SEARCH_DB:
            optimizations.append("ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”")
            optimizations.append("ì¸ë±ìŠ¤ ì¶”ê°€ ì œì•ˆ")
            optimizations.append("ê²°ê³¼ ìºì‹± ì ìš©")
            
        elif tool_type == MCPToolType.GENERATE_MSG:
            optimizations.append("ë©”ì‹œì§€ í…œí”Œë¦¿ ìºì‹±")
            optimizations.append("ìƒì„± ì•Œê³ ë¦¬ì¦˜ ìµœì í™”")
            
        elif tool_type == MCPToolType.SEND_SLACK:
            optimizations.append("ë°°ì¹˜ ì „ì†¡ ìµœì í™”")
            optimizations.append("API í˜¸ì¶œ íš¨ìœ¨í™”")
        
        # ìµœì í™” ì´ë ¥ì— ê¸°ë¡
        self.optimization_history[tool_type].append({
            "timestamp": datetime.now(),
            "execution_time": execution_time,
            "optimizations": optimizations
        })
        
        return optimizations
    
    async def _improve_tool_reliability(self, tool_type: MCPToolType) -> List[str]:
        """ë„êµ¬ ì‹ ë¢°ì„± ê°œì„ """
        optimizations = []
        
        # ìµœê·¼ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        recent_failures = [f for f in self.performance_feedback[-50:] 
                          if f.tool_type == tool_type and not f.success]
        
        error_types = [f.error_type for f in recent_failures if f.error_type]
        
        if error_types:
            # ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¥˜ ìœ í˜•
            most_common_error = max(set(error_types), key=error_types.count)
            
            if "timeout" in most_common_error.lower():
                optimizations.append("íƒ€ì„ì•„ì›ƒ ì„¤ì • ìµœì í™”")
                optimizations.append("ì¬ì‹œë„ ë¡œì§ ê°œì„ ")
                
            elif "connection" in most_common_error.lower():
                optimizations.append("ì—°ê²° í’€ ìµœì í™”")
                optimizations.append("ì—°ê²° ì•ˆì •ì„± ê°œì„ ")
                
            elif "rate limit" in most_common_error.lower():
                optimizations.append("ì†ë„ ì œí•œ ëŒ€ì‘")
                optimizations.append("ìš”ì²­ ê°„ê²© ì¡°ì •")
        
        return optimizations
    
    async def _share_user_preferences(self, user_id: str):
        """ì‚¬ìš©ì ì„ í˜¸ë„ í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ ê³µìœ """
        if user_id in self.user_preferences:
            preferences = self.user_preferences[user_id]
            
            # ë©”ëª¨ë¦¬ì— ì„ í˜¸ë„ ì €ì¥í•˜ì—¬ ë‹¤ë¥¸ ì„¸ì…˜ì—ì„œ í™œìš© ê°€ëŠ¥í•˜ê²Œ í•¨
            await self.memory_service.store_working_memory(
                session_id=f"preferences_{user_id}",
                key="shared_preferences",
                value=preferences,
                ttl=86400  # 24ì‹œê°„
            )
            
            # ì¥ê¸° ë©”ëª¨ë¦¬ì—ë„ ì €ì¥
            preference_text = f"ì‚¬ìš©ì {user_id} ì„ í˜¸ë„: " + ", ".join([
                f"{k}: {v}" for k, v in preferences.items()
            ])
            
            self.memory_service.mem0.add(
                preference_text,
                user_id=user_id,
                metadata={"type": "preference", "shared": True}
            )
    
    async def _store_feedback_in_memory(self, feedback: UserFeedback):
        """í”¼ë“œë°±ì„ ì¥ê¸° ë©”ëª¨ë¦¬ì— ì €ì¥"""
        feedback_text = (
            f"ì‚¬ìš©ì í”¼ë“œë°±: {feedback.feedback_type} - {feedback.content} "
            f"(í‰ì : {feedback.rating})"
        )
        
        self.memory_service.mem0.add(
            feedback_text,
            user_id=feedback.user_id,
            metadata={
                "type": "feedback",
                "feedback_type": feedback.feedback_type,
                "rating": feedback.rating,
                "timestamp": feedback.timestamp.isoformat()
            }
        )
    
    def _calculate_expected_improvements(self, optimizations: List[str]) -> Dict[str, float]:
        """ìµœì í™”ë¡œ ì¸í•œ ì˜ˆìƒ ê°œì„  íš¨ê³¼ ê³„ì‚°"""
        improvements keywords for word in ["ì¡°íšŒ", "ê²€ìƒ‰", "ì°¾ê¸°", "ë°ì´í„°"]):
            suggested_tools.append(MCPToolType.SEARCH_DB)
        
        if any(word in keywords for word in ["ë¹„ìƒ", "ê¸´ê¸‰", "ì‘ê¸‰"]):
            suggested_tools.append(MCPToolType.EMERGENCY_MAIL)
        
        if any(word in